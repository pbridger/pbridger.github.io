<!DOCTYPE html>
<html lang="en" dir=>

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="TorchScript is one of the most important parts of the Pytorch ecosystem, allowing portable, efficient and nearly seamless deployment. With just a few lines of torch.jit code and some simple model changes you can export an asset that runs anywhere libtorch does. It&rsquo;s an important toolset to master if you want to run your models outside the lab at high efficiency. This article is a collection of topics going beyond the basics of your first export.">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification" />
<meta property="og:description" content="TorchScript is one of the most important parts of the Pytorch ecosystem, allowing portable, efficient and nearly seamless deployment. With just a few lines of torch.jit code and some simple model changes you can export an asset that runs anywhere libtorch does. It&rsquo;s an important toolset to master if you want to run your models outside the lab at high efficiency. This article is a collection of topics going beyond the basics of your first export." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://paulbridger.com/posts/mastering-torchscript/" />
<meta property="article:published_time" content="2020-10-29T08:43:23+02:00" />
<meta property="article:modified_time" content="2020-10-29T08:43:23+02:00" />
<title>Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification | paulbridger.com</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.566d6fef220f82a56f11f5f16669dc87e4e7a260974e6b0bcef5e3bf7a2da0aa.css" integrity="sha256-Vm1v7yIPgqVvEfXxZmnch&#43;TnomCXTmsLzvXjv3otoKo=">
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-3441595-4', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
  


<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@paul_bridger">
<meta name="twitter:title" content="Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification">
<meta name="twitter:image" content="https://paulbridger.com/posts/mastering-torchscript/images/pinned_devices.png">





</head>

<body dir=>
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/"><span>paulbridger.com</span>
  </a>
</h2>












  



  
  
  
  

  
  <ul>
    
      
        <li>
          
  
    <a href="/posts/video-analytics-pytorch-pipeline/" class="">A Simple and Flexible Pytorch Video Pipeline</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/posts/video-analytics-pipeline-tuning/" class="">Object Detection from 9 FPS to 650 FPS in 6 Steps</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/posts/video-analytics-deepstream-pipeline/" class="">Object Detection at 1840 FPS with TorchScript, TensorRT and DeepStream</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/posts/mastering-torchscript/" class="active">Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/posts/about/" class="">About Paul Bridger</a>
  

        </li>
      
    
  </ul>
  















</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  <div class="book-toc-div">
<nav id="TableOfContents">
  <ul>
    <li><a href="#intro">Intro</a></li>
    <li><a href="#tracing-vs-scripting">Tracing vs Scripting</a>
      <ul>
        <li><a href="#use-scripting-by-default">Use Scripting by Default</a></li>
        <li><a href="#use-tracing-if-you-must">Use Tracing if You Must</a></li>
        <li><a href="#use-both-together">Use Both Together</a></li>
      </ul>
    </li>
    <li><a href="#device-pinning">Device Pinning</a>
      <ul>
        <li><a href="#performance-and-portability">Performance and Portability</a></li>
        <li><a href="#tensor-subscript-mask-and-indexing-will-pin-devices">Tensor Subscript Mask and Indexing Will Pin Devices</a></li>
      </ul>
    </li>
    <li><a href="#direct-graph-modification">Direct Graph Modification</a></li>
    <li><a href="#rewrite-for-onnxtensorrt-export">Rewrite for ONNX/TensorRT Export</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
<nav>

Have a question or comment?<br>
<a href="https://news.ycombinator.com/item?id=24941642">Join the discussion on Hacker News <img src="/images/y.svg" /></a>

</nav>

</div>


  </aside>
  
 
      </header>

      
      
<article class="markdown">
  <h1>
    <a href="/posts/mastering-torchscript/">Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification</a>
  </h1>
  
  <h5>October 29, 2020</h5>



  
  <div>
    
      <a href="/categories/visual-analytics/">Visual Analytics</a>
  </div>
  

  


  <p><h2 id="intro">
  Intro
  <a class="anchor" href="#intro">#</a>
</h2>
<p>TorchScript is one of the most important parts of the Pytorch ecosystem, allowing portable, efficient and nearly seamless deployment. With just a few lines of <code>torch.jit</code> code and some simple model changes you can export an asset that runs anywhere <code>libtorch</code> does. It&rsquo;s an important toolset to master if you want to run your models outside the lab at high efficiency.</p>
<p>Good <a href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html">introductory material</a> is already available for starting to work with <a href="https://pytorch.org/docs/stable/jit.html">TorchScript</a> including <a href="https://pytorch.org/tutorials/advanced/cpp_export.html">execution in the C++ <code>libtorch</code> runtime</a>, and <a href="https://pytorch.org/docs/stable/jit_language_reference.html">reference material</a> is also provided. This article is a collection of topics going beyond the basics of your first export.</p>
<h2 id="tracing-vs-scripting">
  Tracing vs Scripting
  <a class="anchor" href="#tracing-vs-scripting">#</a>
</h2>
<p>Pytorch provides two methods for generating TorchScript from your model code — tracing and scripting — but which should you use? Let&rsquo;s recap how they work:</p>
<ul>
<li>
<p><a href="https://pytorch.org/docs/stable/generated/torch.jit.trace.html"><strong>Tracing.</strong></a> When using <code>torch.jit.trace</code> you&rsquo;ll provide your model and sample input as arguments. The input will be fed through the model as in regular inference and the executed operations will be traced and recorded into TorchScript. Logical structure will be frozen into the path taken during this sample execution.</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/generated/torch.jit.script.html"><strong>Scripting.</strong></a> When using <code>torch.jit.script</code> you&rsquo;ll simply provide your model as an argument. TorchScript will be generated from the static inspection of the <code>nn.Module</code> contents (recursively).</p>
</li>
</ul>
<p>It&rsquo;s not obvious from the tutorial documentation, but choosing which method to use is a fairly simple and fluid choice:</p>
<h3 id="use-scripting-by-default">
  Use Scripting by Default
  <a class="anchor" href="#use-scripting-by-default">#</a>
</h3>
<p>Because <code>torch.jit.script</code> captures both the operations and full conditional logic of your model, it&rsquo;s a great place to start. If your model doesn&rsquo;t need any <a href="https://pytorch.org/docs/stable/jit_unsupported.html">unsupported Pytorch functionality</a> and has logic restricted to the <a href="https://pytorch.org/docs/stable/jit_builtin_functions.html#python-built-in-functions">supported subset of Python functions</a> and <a href="https://pytorch.org/docs/stable/jit_python_reference.html">syntax</a>, then <code>torch.jit.script</code> should be all you need.</p>
<p>One major advantage of scripting over tracing is that an export is likely to either fail for a well-defined reason — implying a clear code modification — or succeed without warnings.</p>
<blockquote class="book-hint info">
  <p><strong>Unlike Python, TorchScript is Statically Typed</strong></p>
<p>You will need to be consistent about container element datatypes, and be wary of implicit function signatures. A useful practice is to use type hints in method signatures.</p>

</blockquote>

<p>Despite TorchScript&rsquo;s ability to capture conditional logic it does not allow you to run arbitrary Python within <code>libtorch</code> — a popular misconception.</p>
<h3 id="use-tracing-if-you-must">
  Use Tracing if You Must
  <a class="anchor" href="#use-tracing-if-you-must">#</a>
</h3>
<p>There are a few special cases in which <code>torch.jit.trace</code> may be useful:</p>
<ul>
<li>If you are unable to modify the model code — because you do not have access or ownership — you may find scripting the model simply will not work because it uses unsupported Pytorch/Python functionality.</li>
<li>In pursuit of performance or to bake in architectural decisions the logic freezing behavior of tracing might be preferable — similar to inlining C/C++ code.</li>
</ul>
<blockquote class="book-hint warning">
  <p><strong>Pay Close Attention to Tracer Warnings</strong></p>
<p>Due to how tracing can simplify model behavior, each warning should be fully understood and only then ignored (or fixed). Also, be sure to trace in eval mode if you are exporting a model for production inference!</p>

</blockquote>

<h3 id="use-both-together">
  Use Both Together
  <a class="anchor" href="#use-both-together">#</a>
</h3>
<p>Scripted and traced code can be freely mixed, and this is often a great choice. See the existing <a href="https://pytorch.org">pytorch.org</a> documentation for <a href="https://pytorch.org/docs/stable/jit.html#mixing-tracing-and-scripting">details</a> and <a href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html#mixing-scripting-and-tracing">examples</a>.</p>
<h2 id="device-pinning">
  Device Pinning
  <a class="anchor" href="#device-pinning">#</a>
</h2>
<p>If you find yourself using <code>torch.jit.trace</code> on some code, you&rsquo;ll have to actively deal with some of the gotchas or face performance and portability consequences. Besides addressing any warnings Pytorch emits, you&rsquo;ll also need to keep an eye out for device pinning. Just like <code>torch.jit.trace</code> records and freezes conditional logic, it will also trace and make constant the values resulting from this logic — this can include device constants.</p>
<p>Using this sample code:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span></code></pre></div>
<p>If we trace while executing on CPU or GPU we get this TorchScript (scroll to the right on mobile):</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
  <span class="n">_0</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">prim</span><span class="o">.</span><span class="n">NumToTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
  <span class="n">_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">annotate</span><span class="p">(</span><span class="n">number</span><span class="p">,</span> <span class="n">_0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cpu&#34;</span><span class="p">),</span> <span class="n">pin_memory</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_1</span></code></pre></div>
<!-- raw HTML omitted -->
<p>You can see that <code>torch.device(&quot;cpu&quot;)</code> has been inserted as a constant into the generated TorchScript. If we try to get clever with this code:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">)</span></code></pre></div>
<p>Tracing will now result in TorchScript that is pinned to the tracing device. When traced on GPU, we see this:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
    <span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
  <span class="n">_0</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">prim</span><span class="o">.</span><span class="n">NumToTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
  <span class="n">_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">annotate</span><span class="p">(</span><span class="n">number</span><span class="p">,</span> <span class="n">_0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda:0&#34;</span><span class="p">),</span> <span class="n">pin_memory</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_1</span></code></pre></div>
<!-- raw HTML omitted -->
<blockquote class="book-hint warning">
  <p><strong>Tensors Created During Tracing Will Have Their Device Pinned</strong></p>
<p>This can be a significant performance and portability problem.</p>

</blockquote>

<h3 id="performance-and-portability">
  Performance and Portability
  <a class="anchor" href="#performance-and-portability">#</a>
</h3>
<p>If we later deserialize and run this TorchScript in <code>libtorch</code> the <code>arange</code> tensor will always be created on the device that is pinned — <code>torch.device(&quot;cpu&quot;)</code> or <code>torch.device(&quot;cuda:0&quot;)</code> in the examples above. If the rest of the model is running on a different device this can result in costly memory transfers and synchronization.</p>
<p>This device pinning issue extends to multi-GPU scenarios as well. If you have traced and exported a model on <code>cuda:0</code> and then run it on <code>cuda:1</code> you&rsquo;ll see transfers and synchronization between the devices. Not good. Perhaps even worse, if such a model is run in an environment without any CUDA-capable device it will fail since <code>cuda:0</code> doesn&rsquo;t exist.</p>
<blockquote class="book-hint info">
  <p><strong>Replace Tensors Created During Execution With Parameters</strong></p>
<p>Tensors created in the execution path while tracing will have their device pinned. Depending on model logic, these can often be turned into Parameters created during construction.</p>

</blockquote>

<p>An example of the problem looks like this in Nsight Systems:</p>








<a href="/posts/mastering-torchscript/images/pinned_devices.png">
    <figure style="margin: 2rem 0">
        <img style="max-width: 100%; width: auto; height: auto;" src="/posts/mastering-torchscript/images/pinned_devices_hub20c76c57b334a1bd11edec46dac0166_414004_896x580_fill_box_top_2.png" width="896" height="580">
        <figcaption><small></small></figcaption>
    </figure>
</a>

<h3 id="tensor-subscript-mask-and-indexing-will-pin-devices">
  Tensor Subscript Mask and Indexing Will Pin Devices
  <a class="anchor" href="#tensor-subscript-mask-and-indexing-will-pin-devices">#</a>
</h3>
<p>Unlike their more explicit counterparts (<code>masked_select</code> and <code>index_select</code>), using tensor subscripting will pin the mask or indexes to the tracing device:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">[</span><span class="n">X</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span></code></pre></div>
<p>Generates this TorchScript:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
  <span class="n">_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cpu&#34;</span><span class="p">),</span> <span class="n">pin_memory</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
  <span class="n">_1</span> <span class="o">=</span> <span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]],</span> <span class="p">[</span><span class="n">_0</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">_1</span><span class="p">)</span></code></pre></div>
<!-- raw HTML omitted -->
<p>Whereas:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">X</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span></code></pre></div>
<p>Generates this TorchScript:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
  <span class="n">_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">_0</span></code></pre></div>
<!-- raw HTML omitted -->
<p>The same pattern holds for <code>tensor[indexes]</code> and <code>tensor.index_select(0, indexes)</code>. This device pinning carries the same performance and portability risks as noted above.</p>
<blockquote class="book-hint info">
  <p><strong>Replace Tensor Subscripting With <code>masked_select</code> and <code>indexed_select</code></strong></p>
<p>Subscript-based masking and indexing will always pin the tracing device into generated TorchScript. :(</p>

</blockquote>

<h2 id="direct-graph-modification">
  Direct Graph Modification
  <a class="anchor" href="#direct-graph-modification">#</a>
</h2>
<p>Once we&rsquo;ve used <code>torch.jit.script</code> or <code>torch.jit.trace</code> to generate a ScriptModule or ScriptFunction we can use <code>.graph</code>, <code>.inlined_graph</code> or <code>.code</code> to understand exactly what TorchScript has been generated. Though it has an entirely undocumented interface it is possible (and fun) to access and modify the generated TorchScript AST directly via the <code>.graph</code> method.</p>
<p>The most useful parts of the API are defined in <a href="https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/python/python_ir.cpp">torch/csrc/jit/python/python_ir.cpp</a>. As you can see, all the basic functionality is present for finding and changing the graph nodes you want. If you change nodes or arguments and then persist the module your subsequent TorchScript load and inference will reflect your changes, though modules cannot be changed recursively in this way (<code>torch.jit.freeze</code> can be useful here).</p>
<p>An example of the kind of graph modification that is possible:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">undevice</span><span class="p">(</span><span class="n">tsc</span><span class="p">):</span>
    <span class="c1"># use ::to variant which does not hardcode device</span>
    <span class="k">for</span> <span class="n">to_node</span> <span class="ow">in</span> <span class="n">tsc</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">findAllNodes</span><span class="p">(</span><span class="s1">&#39;aten::to&#39;</span><span class="p">):</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">layout</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">pin_mem</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">copy</span><span class="p">,</span> <span class="n">mem_format</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">to_node</span><span class="o">.</span><span class="n">inputs</span><span class="p">())</span>
        <span class="n">to_node</span><span class="o">.</span><span class="n">removeAllInputs</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">copy</span><span class="p">,</span> <span class="n">mem_format</span><span class="p">]:</span>
            <span class="n">to_node</span><span class="o">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">constant</span> <span class="ow">in</span> <span class="n">tsc</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">findAllNodes</span><span class="p">(</span><span class="s1">&#39;prim::Constant&#39;</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">constant</span><span class="o">.</span><span class="n">hasUses</span><span class="p">():</span>
            <span class="n">constant</span><span class="o">.</span><span class="n">destroy</span><span class="p">()</span></code></pre></div>
<p>The above code will modify a traced graph, changing <code>aten::to</code> to use an overload which doesn&rsquo;t change memory location.</p>
<p>But what is this really useful for? As an undocumented API you&rsquo;d be unwise to use this capability in a production pipeline unless you like maintenance coding. I would only recommend it for research, as in the above example which I used to understand and profile the transfer/synchronization behavior of tensor subscripting.</p>
<blockquote class="book-hint info">
  <p><strong>Don&rsquo;t Bother With Direct Graph Modification</strong></p>
<p>For legitimate production use-cases you can almost always find a way to modify your model code to generate the TorchScript you want.</p>

</blockquote>

<h2 id="rewrite-for-onnxtensorrt-export">
  Rewrite for ONNX/TensorRT Export
  <a class="anchor" href="#rewrite-for-onnxtensorrt-export">#</a>
</h2>
<p>You can get some <a href="/posts/video-analytics-deepstream-pipeline/#stage-3-hacked-deepstream-mdash-80-tensorrt-20-torchscript">awesome results with TensorRT</a> but exporting a model from Pytorch to TensorRT is far from a sure thing. The export path to ONNX and then to TensorRT can fail due to missing or incompatible operations at either step and this can be frustrating.</p>
<p>After the obligatory Google search, I&rsquo;ve found a reasonable hail-mary approach is to rewrite your tensor processing code to avoid unsupported operators. I can&rsquo;t give general advice for this but let me show you an example of how this can be possible: <code>repeat_interleave</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">RI</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">repeat</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">repeat</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">RI</span><span class="p">(),</span> <span class="n">inputs</span><span class="p">,</span> <span class="s1">&#39;please_work.onnx&#39;</span><span class="p">,</span> <span class="n">opset_version</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span></code></pre></div>
<p>Doesn&rsquo;t work:</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">RuntimeError: Exporting the operator repeat_interleave to ONNX opset version <span class="m">11</span> is not supported. Please open a bug to request ONNX <span class="nb">export</span> support <span class="k">for</span> the missing operator.</code></pre></div>
<p>However, the behavior of <code>repeat_interleave</code> with a fixed <code>dim</code> argument can be replicated in a form that will export to ONNX (but good luck passing code review):</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">RW</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">repeat</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">repeat</span><span class="p">,</span> <span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span></code></pre></div>
<p>N.B. The above code is only equivalent to <code>repeat_interleave(X, dim=0)</code> though it can be adapted for any fixed dim.</p>
<p>The same approach can be taken to work around incomplete support in TensorRT, which is far more prevalent in my experience.</p>
<h2 id="conclusion">
  Conclusion
  <a class="anchor" href="#conclusion">#</a>
</h2>
<p>Efficient and portable Pytorch production deployment used to be almost impossible, but the introduction and continued evolution of TorchScript has been great for the ecosystem. There are still a few rough edges and tricks, and I hope you&rsquo;ve found something new or useful in the topics above.</p>
<p>If there is some major concern or problem you&rsquo;re having with TorchScript or Pytorch production deployment please get in touch — I&rsquo;m always looking for new areas to research.</p>
</p>
</article>
 
      <article class="markdown social">
    <hr>
    <section class="social-share">
      
<ul class="share-icons">
  
    <li>
        <a href="https://twitter.com/intent/follow?ref_src=twsrc%5Etfw&region=follow_link&screen_name=paul_bridger&tw_p=followbutton" target="_blank" rel="noopener" aria-label="Follow on Twitter" class="share-btn twitter">
            <svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path fill="#ffffff" d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5 0-78.8 35.3-78.8 78.8 0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3-6.7 11.6-10.6 25.2-10.6 39.6 0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1 0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4 0-12.6-.4-18.8-1.1 34.9 22.4 76.3 35.4 120.8 35.4 144.9 0 224.1-120 224.1-224.1 0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>

            <p>Twitter</p>
        </a>
    </li>
    <li class="share-label">
        <a href="https://twitter.com/intent/follow?ref_src=twsrc%5Etfw&region=follow_link&screen_name=paul_bridger&tw_p=followbutton" target="_blank" rel="noopener" aria-label="Follow on Twitter">
            Follow on Twitter
        </a>
    </li>
</ul>

<ul class="share-icons">
    
    <li>
        <a href="https://www.linkedin.com/in/paulbridger/" target="_blank" rel="noopener" aria-label="Connect on LinkedIn" class="share-btn linkedin">
            <svg class="widget-social__link-icon icon icon-linkedin" width="24" height="24" viewBox="0 0 352 352"><path fill="#ffffff" d="M0,40v272c0,21.9,18.1,40,40,40h272c21.9,0,40-18.1,40-40V40c0-21.9-18.1-40-40-40H40C18.1,0,0,18.1,0,40z M312,32 c4.6,0,8,3.4,8,8v272c0,4.6-3.4,8-8,8H40c-4.6,0-8-3.4-8-8V40c0-4.6,3.4-8,8-8H312z M59.5,87c0,15.2,12.3,27.5,27.5,27.5 c15.2,0,27.5-12.3,27.5-27.5c0-15.2-12.3-27.5-27.5-27.5C71.8,59.5,59.5,71.8,59.5,87z M187,157h-1v-21h-45v152h47v-75 c0-19.8,3.9-39,28.5-39c24.2,0,24.5,22.4,24.5,40v74h47v-83.5c0-40.9-8.7-72-56.5-72C208.5,132.5,193.3,145.1,187,157z M64,288h47.5 V136H64V288z"/></svg>

            <p>LinkedIn</p>
        </a>
    </li>
    <li class="share-label">
        <a href="https://www.linkedin.com/in/paulbridger/" target="_blank" rel="noopener" aria-label="Connect on LinkedIn">
            Connect on LinkedIn
        </a>
    </li>
</ul>

<ul class="share-icons">
    
    <li>
        <a href="mailto:paul@paulbridger.com" target="_blank" class="share-btn email">
            <svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path fill="#ffffff" d="m0 16v256 16h16 384 16v-16-256-16h-16-384-16zm347 16-139 92.5-139-92.5zm-148 125.5 9 5.5 9-5.5 167-111.5v210h-352v-210z"/></svg>

            <p>Email</p>
        </a>
    </li>
    <li class="share-label">
        <a href="mailto:paul@paulbridger.com" target="_blank">
            Contact by Email
        </a>
    </li>
</ul>


<ul class="share-icons">
    <li>
        <a href="https://news.ycombinator.com/item?id=24941642" class="share-btn yc">
            <svg class="widget-social__link-icon icon icon-yc" width="24" height="24" viewBox="3 3 24 24">
	<path fill="#FFFFFF" d="M14,17L8.8,7.3h2.4l3,6.1c0,0.1,0.1,0.2,0.2,0.3c0.1,0.1,0.1,0.2,0.2,0.4
		c0,0,0.1,0.1,0.1,0.1c0,0,0,0.1,0,0.1c0.1,0.2,0.1,0.3,0.2,0.5c0.1,0.1,0.1,0.3,0.2,0.4c0.1-0.3,0.3-0.5,0.4-0.9
		c0.1-0.3,0.3-0.6,0.5-0.9L19,7.3h2.2L16,17.1v6.2h-2V17z"/>
</svg>

            <p>Hacker News</p>
        </a>
    </li>
    <li class="share-label">
        <a href="https://news.ycombinator.com/item?id=24941642" target="_blank">
            Discuss on HN
        </a>
    </li>
</ul>


    </section>

</article>


      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>

 
        <hr>
<b>&copy; Paul Bridger 2020</b>

      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      
  <div class="book-toc-div">
<nav id="TableOfContents">
  <ul>
    <li><a href="#intro">Intro</a></li>
    <li><a href="#tracing-vs-scripting">Tracing vs Scripting</a>
      <ul>
        <li><a href="#use-scripting-by-default">Use Scripting by Default</a></li>
        <li><a href="#use-tracing-if-you-must">Use Tracing if You Must</a></li>
        <li><a href="#use-both-together">Use Both Together</a></li>
      </ul>
    </li>
    <li><a href="#device-pinning">Device Pinning</a>
      <ul>
        <li><a href="#performance-and-portability">Performance and Portability</a></li>
        <li><a href="#tensor-subscript-mask-and-indexing-will-pin-devices">Tensor Subscript Mask and Indexing Will Pin Devices</a></li>
      </ul>
    </li>
    <li><a href="#direct-graph-modification">Direct Graph Modification</a></li>
    <li><a href="#rewrite-for-onnxtensorrt-export">Rewrite for ONNX/TensorRT Export</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
<nav>

Have a question or comment?<br>
<a href="https://news.ycombinator.com/item?id=24941642">Join the discussion on Hacker News <img src="/images/y.svg" /></a>

</nav>

</div>

 
    </aside>
    
  </main>

  
</body>

</html>












