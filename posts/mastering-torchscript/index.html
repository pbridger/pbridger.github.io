<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="TorchScript is one of the most important parts of the Pytorch ecosystem, allowing portable, efficient and nearly seamless deployment. With just a few lines of torch.jit code and some simple model changes you can export an asset that runs anywhere libtorch does. It&rsquo;s an important toolset to master if you want to run your models outside the lab at high efficiency. This article is a collection of topics going beyond the basics of your first export."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification"><meta property="og:description" content="TorchScript is one of the most important parts of the Pytorch ecosystem, allowing portable, efficient and nearly seamless deployment. With just a few lines of torch.jit code and some simple model changes you can export an asset that runs anywhere libtorch does. It&rsquo;s an important toolset to master if you want to run your models outside the lab at high efficiency. This article is a collection of topics going beyond the basics of your first export."><meta property="og:type" content="article"><meta property="og:url" content="https://paulbridger.com/posts/mastering-torchscript/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-10-29T08:43:23+02:00"><meta property="article:modified_time" content="2020-10-29T08:43:23+02:00"><title>Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification | paulbridger.com</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.23da3cf037fbe70fe3726e8cd507f54b2e18ea698dec0552430692be6eb655d8.css integrity="sha256-I9o88Df75w/jcm6M1Qf1Sy4Y6mmN7AVSQwaSvm62Vdg=" crossorigin=anonymous><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-3441595-4','auto'),ga('send','pageview'))</script><script async src=https://www.google-analytics.com/analytics.js></script><meta name=twitter:card content="summary"><meta name=twitter:site content="@paul_bridger"><meta name=twitter:title content="Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification"><meta name=twitter:image content="https://paulbridger.com/posts/mastering-torchscript/images/pinned_devices.png"></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a href=/><span>paulbridger.com</span></a></h2><ul><li><a href=https://paulbridger.com/posts/nsight-systems-systematic-optimization/>Solving Machine Learning Performance Anti-Patterns: a Systematic Approach</a></li><li><a href=https://paulbridger.com/posts/video-analytics-pytorch-pipeline/>A Simple and Flexible Pytorch Video Pipeline</a></li><li><a href=https://paulbridger.com/posts/video-analytics-pipeline-tuning/>Object Detection from 9 FPS to 650 FPS in 6 Steps</a></li><li><a href=https://paulbridger.com/posts/video-analytics-deepstream-pipeline/>Object Detection at 1840 FPS with TorchScript, TensorRT and DeepStream</a></li><li><a href=https://paulbridger.com/posts/tensorrt-object-detection-quantized/>Object Detection at 2530 FPS with TensorRT and 8-Bit Quantization</a></li><li><a href=https://paulbridger.com/posts/mastering-torchscript/ class=active>Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification</a></li><li><a href=https://paulbridger.com/posts/consulting/>Consulting / Hire Me</a></li><li><a href=https://paulbridger.com/posts/about/>About Paul Bridger</a></li></ul></nav><script>(function(){var a=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><div class=book-toc-div><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#tracing-vs-scripting>Tracing vs Scripting</a><ul><li><a href=#use-scripting-by-default>Use Scripting by Default</a></li><li><a href=#use-tracing-if-you-must>Use Tracing if You Must</a></li><li><a href=#use-both-together>Use Both Together</a></li></ul></li><li><a href=#device-pinning>Device Pinning</a><ul><li><a href=#performance-and-portability>Performance and Portability</a></li><li><a href=#tensor-subscript-mask-and-indexing-will-pin-devices>Tensor Subscript Mask and Indexing Will Pin Devices</a></li></ul></li><li><a href=#direct-graph-modification>Direct Graph Modification</a></li><li><a href=#rewrite-for-onnxtensorrt-export>Rewrite for ONNX/TensorRT Export</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav><nav></nav></div></aside></header><article class=markdown><h1><a href=/posts/mastering-torchscript/>Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification</a></h1><h5>October 29, 2020</h5><div><a href=/categories/machine-learning-productionization/>Machine Learning Productionization</a></div><div><a href=/tags/pytorch/>Pytorch</a>,
<a href=/tags/torchscript/>TorchScript</a>,
<a href=/tags/tensorrt/>TensorRT</a>,
<a href=/tags/onnx/>ONNX</a>,
<a href=/tags/nsight-systems/>Nsight Systems</a></div><h2 id=intro>Intro
<a class=anchor href=#intro>#</a></h2><p>TorchScript is one of the most important parts of the Pytorch ecosystem, allowing portable, efficient and nearly seamless deployment. With just a few lines of <code>torch.jit</code> code and some simple model changes you can export an asset that runs anywhere <code>libtorch</code> does. It&rsquo;s an important toolset to master if you want to run your models outside the lab at high efficiency.</p><p>Good <a href=https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html>introductory material</a> is already available for starting to work with <a href=https://pytorch.org/docs/stable/jit.html>TorchScript</a> including <a href=https://pytorch.org/tutorials/advanced/cpp_export.html>execution in the C++ <code>libtorch</code> runtime</a>, and <a href=https://pytorch.org/docs/stable/jit_language_reference.html>reference material</a> is also provided. This article is a collection of topics going beyond the basics of your first export.</p><h2 id=tracing-vs-scripting>Tracing vs Scripting
<a class=anchor href=#tracing-vs-scripting>#</a></h2><p>Pytorch provides two methods for generating TorchScript from your model code — tracing and scripting — but which should you use? Let&rsquo;s recap how they work:</p><ul><li><p><a href=https://pytorch.org/docs/stable/generated/torch.jit.trace.html><strong>Tracing.</strong></a> When using <code>torch.jit.trace</code> you&rsquo;ll provide your model and sample input as arguments. The input will be fed through the model as in regular inference and the executed operations will be traced and recorded into TorchScript. Logical structure will be frozen into the path taken during this sample execution.</p></li><li><p><a href=https://pytorch.org/docs/stable/generated/torch.jit.script.html><strong>Scripting.</strong></a> When using <code>torch.jit.script</code> you&rsquo;ll simply provide your model as an argument. TorchScript will be generated from the static inspection of the <code>nn.Module</code> contents (recursively).</p></li></ul><p>It&rsquo;s not obvious from the tutorial documentation, but choosing which method to use is a fairly simple and fluid choice:</p><h3 id=use-scripting-by-default>Use Scripting by Default
<a class=anchor href=#use-scripting-by-default>#</a></h3><p>Because <code>torch.jit.script</code> captures both the operations and full conditional logic of your model, it&rsquo;s a great place to start. If your model doesn&rsquo;t need any <a href=https://pytorch.org/docs/stable/jit_unsupported.html>unsupported Pytorch functionality</a> and has logic restricted to the <a href=https://pytorch.org/docs/stable/jit_builtin_functions.html#python-built-in-functions>supported subset of Python functions</a> and <a href=https://pytorch.org/docs/stable/jit_python_reference.html>syntax</a>, then <code>torch.jit.script</code> should be all you need.</p><p>One major advantage of scripting over tracing is that an export is likely to either fail for a well-defined reason — implying a clear code modification — or succeed without warnings.</p><blockquote class="book-hint info"><p><strong>Unlike Python, TorchScript is Statically Typed</strong></p><p>You will need to be consistent about container element datatypes, and be wary of implicit function signatures. A useful practice is to use type hints in method signatures.</p></blockquote><p>Despite TorchScript&rsquo;s ability to capture conditional logic it does not allow you to run arbitrary Python within <code>libtorch</code> — a popular misconception.</p><h3 id=use-tracing-if-you-must>Use Tracing if You Must
<a class=anchor href=#use-tracing-if-you-must>#</a></h3><p>There are a few special cases in which <code>torch.jit.trace</code> may be useful:</p><ul><li>If you are unable to modify the model code — because you do not have access or ownership — you may find scripting the model simply will not work because it uses unsupported Pytorch/Python functionality.</li><li>In pursuit of performance or to bake in architectural decisions the logic freezing behavior of tracing might be preferable — similar to inlining C/C++ code.</li></ul><blockquote class="book-hint warning"><p><strong>Pay Close Attention to Tracer Warnings</strong></p><p>Due to how tracing can simplify model behavior, each warning should be fully understood and only then ignored (or fixed). Also, be sure to trace in eval mode if you are exporting a model for production inference!</p></blockquote><h3 id=use-both-together>Use Both Together
<a class=anchor href=#use-both-together>#</a></h3><p>Scripted and traced code can be freely mixed, and this is often a great choice. See the existing <a href=https://pytorch.org>pytorch.org</a> documentation for <a href=https://pytorch.org/docs/stable/jit.html#mixing-tracing-and-scripting>details</a> and <a href=https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html#mixing-scripting-and-tracing>examples</a>.</p><h2 id=device-pinning>Device Pinning
<a class=anchor href=#device-pinning>#</a></h2><p>If you find yourself using <code>torch.jit.trace</code> on some code, you&rsquo;ll have to actively deal with some of the gotchas or face performance and portability consequences. Besides addressing any warnings Pytorch emits, you&rsquo;ll also need to keep an eye out for device pinning. Just like <code>torch.jit.trace</code> records and freezes conditional logic, it will also trace and make constant the values resulting from this logic — this can include device constants.</p><p>Using this sample code:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span></code></pre></div><p>If we trace while executing on CPU or GPU we get this TorchScript (scroll to the right on mobile):</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>X</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
  <span class=n>_0</span> <span class=o>=</span> <span class=n>ops</span><span class=o>.</span><span class=n>prim</span><span class=o>.</span><span class=n>NumToTensor</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span>
  <span class=n>_1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>annotate</span><span class=p>(</span><span class=n>number</span><span class=p>,</span> <span class=n>_0</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=bp>None</span><span class=p>,</span> <span class=n>layout</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cpu&#34;</span><span class=p>),</span> <span class=n>pin_memory</span><span class=o>=</span><span class=bp>False</span><span class=p>)</span>
  <span class=k>return</span> <span class=n>_1</span></code></pre></div><p>You can see that <code>torch.device("cpu")</code> has been inserted as a constant into the generated TorchScript. If we try to get clever with this code:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=n>device</span><span class=o>=</span><span class=n>X</span><span class=o>.</span><span class=n>device</span><span class=p>)</span></code></pre></div><p>Tracing will now result in TorchScript that is pinned to the tracing device. When traced on GPU, we see this:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span>
    <span class=n>X</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
  <span class=n>_0</span> <span class=o>=</span> <span class=n>ops</span><span class=o>.</span><span class=n>prim</span><span class=o>.</span><span class=n>NumToTensor</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span>
  <span class=n>_1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>annotate</span><span class=p>(</span><span class=n>number</span><span class=p>,</span> <span class=n>_0</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=bp>None</span><span class=p>,</span> <span class=n>layout</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cuda:0&#34;</span><span class=p>),</span> <span class=n>pin_memory</span><span class=o>=</span><span class=bp>False</span><span class=p>)</span>
  <span class=k>return</span> <span class=n>_1</span></code></pre></div><blockquote class="book-hint warning"><p><strong>Tensors Created During Tracing Will Have Their Device Pinned</strong></p><p>This can be a significant performance and portability problem.</p></blockquote><h3 id=performance-and-portability>Performance and Portability
<a class=anchor href=#performance-and-portability>#</a></h3><p>If we later deserialize and run this TorchScript in <code>libtorch</code> the <code>arange</code> tensor will always be created on the device that is pinned — <code>torch.device("cpu")</code> or <code>torch.device("cuda:0")</code> in the examples above. If the rest of the model is running on a different device this can result in costly memory transfers and synchronization.</p><p>This device pinning issue extends to multi-GPU scenarios as well. If you have traced and exported a model on <code>cuda:0</code> and then run it on <code>cuda:1</code> you&rsquo;ll see transfers and synchronization between the devices. Not good. Perhaps even worse, if such a model is run in an environment without any CUDA-capable device it will fail since <code>cuda:0</code> doesn&rsquo;t exist.</p><blockquote class="book-hint info"><p><strong>Replace Tensors Created During Execution With Parameters</strong></p><p>Tensors created in the execution path while tracing will have their device pinned. Depending on model logic, these can often be turned into Parameters created during construction.</p></blockquote><p>An example of the problem looks like this in Nsight Systems:</p><a href=/posts/mastering-torchscript/images/pinned_devices.png><figure style="margin:2rem 0"><img style=max-width:100%;width:auto;height:auto src=/posts/mastering-torchscript/images/pinned_devices_hub20c76c57b334a1bd11edec46dac0166_414004_896x580_fill_box_top_2.png width=896 height=580><figcaption><small></small></figcaption></figure></a><h3 id=tensor-subscript-mask-and-indexing-will-pin-devices>Tensor Subscript Mask and Indexing Will Pin Devices
<a class=anchor href=#tensor-subscript-mask-and-indexing-will-pin-devices>#</a></h3><p>Unlike their more explicit counterparts (<code>masked_select</code> and <code>index_select</code>), using tensor subscripting will pin the mask or indexes to the tracing device:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>X</span><span class=p>[</span><span class=n>X</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>]</span></code></pre></div><p>Generates this TorchScript:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>X</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
  <span class=n>_0</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>gt</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=mi>11</span><span class=p>,</span> <span class=n>layout</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cpu&#34;</span><span class=p>),</span> <span class=n>pin_memory</span><span class=o>=</span><span class=bp>False</span><span class=p>,</span> <span class=n>non_blocking</span><span class=o>=</span><span class=bp>False</span><span class=p>,</span> <span class=n>copy</span><span class=o>=</span><span class=bp>False</span><span class=p>,</span> <span class=n>memory_format</span><span class=o>=</span><span class=bp>None</span><span class=p>)</span>
  <span class=n>_1</span> <span class=o>=</span> <span class=n>annotate</span><span class=p>(</span><span class=n>List</span><span class=p>[</span><span class=n>Optional</span><span class=p>[</span><span class=n>Tensor</span><span class=p>]],</span> <span class=p>[</span><span class=n>_0</span><span class=p>])</span>
  <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>index</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>_1</span><span class=p>)</span></code></pre></div><p>Whereas:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>X</span><span class=o>.</span><span class=n>masked_select</span><span class=p>(</span><span class=n>X</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>)</span></code></pre></div><p>Generates this TorchScript:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>X</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
  <span class=n>_0</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>masked_select</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>gt</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
  <span class=k>return</span> <span class=n>_0</span></code></pre></div><p>The same pattern holds for <code>tensor[indexes]</code> and <code>tensor.index_select(0, indexes)</code>. This device pinning carries the same performance and portability risks as noted above.</p><blockquote class="book-hint info"><p><strong>Replace Tensor Subscripting With <code>masked_select</code> and <code>index_select</code></strong></p><p>Subscript-based masking and indexing will always pin the tracing device into generated TorchScript. :(</p></blockquote><h2 id=direct-graph-modification>Direct Graph Modification
<a class=anchor href=#direct-graph-modification>#</a></h2><p>Once we&rsquo;ve used <code>torch.jit.script</code> or <code>torch.jit.trace</code> to generate a ScriptModule or ScriptFunction we can use <code>.graph</code>, <code>.inlined_graph</code> or <code>.code</code> to understand exactly what TorchScript has been generated. Though it has an entirely undocumented interface it is possible (and fun) to access and modify the generated TorchScript AST directly via the <code>.graph</code> method.</p><p>The most useful parts of the API are defined in <a href=https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/python/python_ir.cpp>torch/csrc/jit/python/python_ir.cpp</a>. As you can see, all the basic functionality is present for finding and changing the graph nodes you want. If you change nodes or arguments and then persist the module your subsequent TorchScript load and inference will reflect your changes, though modules cannot be changed recursively in this way (<code>torch.jit.freeze</code> can be useful here).</p><p>An example of the kind of graph modification that is possible:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>undevice</span><span class=p>(</span><span class=n>tsc</span><span class=p>):</span>
    <span class=c1># use ::to variant which does not hardcode device</span>
    <span class=k>for</span> <span class=n>to_node</span> <span class=ow>in</span> <span class=n>tsc</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>findAllNodes</span><span class=p>(</span><span class=s1>&#39;aten::to&#39;</span><span class=p>):</span>
        <span class=n>i</span><span class=p>,</span> <span class=n>dtype</span><span class=p>,</span> <span class=n>layout</span><span class=p>,</span> <span class=n>device</span><span class=p>,</span> <span class=n>pin_mem</span><span class=p>,</span> <span class=n>non_blocking</span><span class=p>,</span> <span class=n>copy</span><span class=p>,</span> <span class=n>mem_format</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>to_node</span><span class=o>.</span><span class=n>inputs</span><span class=p>())</span>
        <span class=n>to_node</span><span class=o>.</span><span class=n>removeAllInputs</span><span class=p>()</span>
        <span class=k>for</span> <span class=n>a</span> <span class=ow>in</span> <span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>dtype</span><span class=p>,</span> <span class=n>non_blocking</span><span class=p>,</span> <span class=n>copy</span><span class=p>,</span> <span class=n>mem_format</span><span class=p>]:</span>
            <span class=n>to_node</span><span class=o>.</span><span class=n>addInput</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>

    <span class=k>for</span> <span class=n>constant</span> <span class=ow>in</span> <span class=n>tsc</span><span class=o>.</span><span class=n>graph</span><span class=o>.</span><span class=n>findAllNodes</span><span class=p>(</span><span class=s1>&#39;prim::Constant&#39;</span><span class=p>):</span>
        <span class=k>if</span> <span class=ow>not</span> <span class=n>constant</span><span class=o>.</span><span class=n>hasUses</span><span class=p>():</span>
            <span class=n>constant</span><span class=o>.</span><span class=n>destroy</span><span class=p>()</span></code></pre></div><p>The above code will modify a traced graph, changing <code>aten::to</code> to use an overload which doesn&rsquo;t change memory location.</p><p>But what is this really useful for? As an undocumented API you&rsquo;d be unwise to use this capability in a production pipeline unless you like maintenance coding. I would only recommend it for research, as in the above example which I used to understand and profile the transfer/synchronization behavior of tensor subscripting.</p><blockquote class="book-hint info"><p><strong>Don&rsquo;t Bother With Direct Graph Modification</strong></p><p>For legitimate production use-cases you can almost always find a way to modify your model code to generate the TorchScript you want.</p></blockquote><h2 id=rewrite-for-onnxtensorrt-export>Rewrite for ONNX/TensorRT Export
<a class=anchor href=#rewrite-for-onnxtensorrt-export>#</a></h2><p>You can get some <a href=/posts/video-analytics-deepstream-pipeline/#stage-3-hacked-deepstream-mdash-80-tensorrt-20-torchscript>awesome results with TensorRT</a> but exporting a model from Pytorch to TensorRT is far from a sure thing. The export path to ONNX and then to TensorRT can fail due to missing or incompatible operations at either step and this can be frustrating.</p><p>After the obligatory Google search, I&rsquo;ve found a reasonable hail-mary approach is to rewrite your tensor processing code to avoid unsupported operators. I can&rsquo;t give general advice for this but let me show you an example of how this can be possible: <code>repeat_interleave</code>.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>RI</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>repeat</span><span class=p>):</span>
        <span class=k>return</span> <span class=n>X</span><span class=o>.</span><span class=n>repeat_interleave</span><span class=p>(</span><span class=n>repeat</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>

<span class=n>inputs</span> <span class=o>=</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>5</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mi>3</span><span class=p>))</span>
<span class=n>torch</span><span class=o>.</span><span class=n>onnx</span><span class=o>.</span><span class=n>export</span><span class=p>(</span><span class=n>RI</span><span class=p>(),</span> <span class=n>inputs</span><span class=p>,</span> <span class=s1>&#39;please_work.onnx&#39;</span><span class=p>,</span> <span class=n>opset_version</span><span class=o>=</span><span class=mi>11</span><span class=p>)</span></code></pre></div><p>Doesn&rsquo;t work:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>RuntimeError: Exporting the operator repeat_interleave to ONNX opset version <span class=m>11</span> is not supported. Please open a bug to request ONNX <span class=nb>export</span> support <span class=k>for</span> the missing operator.</code></pre></div><p>However, the behavior of <code>repeat_interleave</code> with a fixed <code>dim</code> argument can be replicated in a form that will export to ONNX (but good luck passing code review):</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>RW</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>repeat</span><span class=p>):</span>
        <span class=n>X</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=o>*</span><span class=n>X</span><span class=o>.</span><span class=n>size</span><span class=p>())</span><span class=o>.</span><span class=n>expand</span><span class=p>(</span><span class=n>repeat</span><span class=p>,</span> <span class=o>*</span><span class=n>X</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>unbind</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>))</span></code></pre></div><p>N.B. The above code is only equivalent to <code>repeat_interleave(X, dim=0)</code> though it can be adapted for any fixed dim.</p><p>The same approach can be taken to work around incomplete support in TensorRT, which is far more prevalent in my experience.</p><h2 id=conclusion>Conclusion
<a class=anchor href=#conclusion>#</a></h2><p>Efficient and portable Pytorch production deployment used to be almost impossible, but the introduction and continued evolution of TorchScript has been great for the ecosystem. There are still a few rough edges and tricks, and I hope you&rsquo;ve found something new or useful in the topics above.</p><p>If there is some major concern or problem you&rsquo;re having with TorchScript or Pytorch production deployment please get in touch — I&rsquo;m always looking for new areas to research.</p></article><article class="markdown social"><hr><section class=social-share><ul class=share-icons><li><a href="https://twitter.com/intent/follow?ref_src=twsrc%5Etfw&region=follow_link&screen_name=paul_bridger&tw_p=followbutton" target=_blank rel=noopener aria-label="Follow on Twitter" class="share-btn twitter"><svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path fill="#fff" d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5.0-78.8 35.3-78.8 78.8.0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3C20 26 16.1 39.6 16.1 54c0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1.0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4.0-12.6-.4-18.8-1.1C34.9 299 76.3 312 120.8 312c144.9.0 224.1-120 224.1-224.1.0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg><p>Twitter</p></a></li><li class=share-label><a href="https://twitter.com/intent/follow?ref_src=twsrc%5Etfw&region=follow_link&screen_name=paul_bridger&tw_p=followbutton" target=_blank rel=noopener aria-label="Follow on Twitter">Follow on Twitter</a></li></ul><ul class=share-icons><li><a href=https://www.linkedin.com/in/paulbridger/ target=_blank rel=noopener aria-label="Connect on LinkedIn" class="share-btn linkedin"><svg class="widget-social__link-icon icon icon-linkedin" width="24" height="24" viewBox="0 0 352 352"><path fill="#fff" d="M0 40v272c0 21.9 18.1 40 40 40h272c21.9.0 40-18.1 40-40V40c0-21.9-18.1-40-40-40H40C18.1.0.0 18.1.0 40zm312-8c4.6.0 8 3.4 8 8v272c0 4.6-3.4 8-8 8H40c-4.6.0-8-3.4-8-8V40c0-4.6 3.4-8 8-8H312zM59.5 87c0 15.2 12.3 27.5 27.5 27.5s27.5-12.3 27.5-27.5S102.2 59.5 87 59.5 59.5 71.8 59.5 87zM187 157h-1v-21h-45v152h47v-75c0-19.8 3.9-39 28.5-39 24.2.0 24.5 22.4 24.5 40v74h47v-83.5c0-40.9-8.7-72-56.5-72-23 0-38.2 12.6-44.5 24.5zM64 288h47.5V136H64V288z"/></svg><p>LinkedIn</p></a></li><li class=share-label><a href=https://www.linkedin.com/in/paulbridger/ target=_blank rel=noopener aria-label="Connect on LinkedIn">Connect on LinkedIn</a></li></ul><ul class=share-icons><li><a href=mailto:paul@paulbridger.com target=_blank class="share-btn email"><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path fill="#fff" d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><p>Email</p></a></li><li class=share-label><a href=mailto:paul@paulbridger.com target=_blank>Contact by Email</a></li></ul></section></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function a(c){const a=window.getSelection(),b=document.createRange();b.selectNodeContents(c),a.removeAllRanges(),a.addRange(b)}document.querySelectorAll("pre code").forEach(b=>{b.addEventListener("click",function(c){a(b.parentElement),navigator.clipboard&&navigator.clipboard.writeText(b.parentElement.textContent)})})})()</script><hr><b>&copy; Paul Bridger 2020</b></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><div class=book-toc-div><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#tracing-vs-scripting>Tracing vs Scripting</a><ul><li><a href=#use-scripting-by-default>Use Scripting by Default</a></li><li><a href=#use-tracing-if-you-must>Use Tracing if You Must</a></li><li><a href=#use-both-together>Use Both Together</a></li></ul></li><li><a href=#device-pinning>Device Pinning</a><ul><li><a href=#performance-and-portability>Performance and Portability</a></li><li><a href=#tensor-subscript-mask-and-indexing-will-pin-devices>Tensor Subscript Mask and Indexing Will Pin Devices</a></li></ul></li><li><a href=#direct-graph-modification>Direct Graph Modification</a></li><li><a href=#rewrite-for-onnxtensorrt-export>Rewrite for ONNX/TensorRT Export</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav><nav></nav></div></div></aside></main></body></html>