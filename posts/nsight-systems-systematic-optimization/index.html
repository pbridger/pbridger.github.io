<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="This article is a high-level introduction to an efficient worfklow for optimizing runtime performance of machine learning systems running on the GPU. Using traces from Nsight Systems to show real production scenarios, I introduce a set of common utilization patterns and outline effective approaches to improve performance."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Solving Machine Learning Performance Anti-Patterns: a Systematic Approach"><meta property="og:description" content="This article is a high-level introduction to an efficient worfklow for optimizing runtime performance of machine learning systems running on the GPU. Using traces from Nsight Systems to show real production scenarios, I introduce a set of common utilization patterns and outline effective approaches to improve performance."><meta property="og:type" content="article"><meta property="og:url" content="https://paulbridger.com/posts/nsight-systems-systematic-optimization/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-06-24T00:02:03+02:00"><meta property="article:modified_time" content="2021-06-24T00:02:03+02:00"><title>Solving Machine Learning Performance Anti-Patterns: a Systematic Approach | paulbridger.com</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.23da3cf037fbe70fe3726e8cd507f54b2e18ea698dec0552430692be6eb655d8.css integrity="sha256-I9o88Df75w/jcm6M1Qf1Sy4Y6mmN7AVSQwaSvm62Vdg=" crossorigin=anonymous><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-3441595-4','auto'),ga('send','pageview'))</script><script async src=https://www.google-analytics.com/analytics.js></script><meta name=twitter:card content="summary"><meta name=twitter:site content="@paul_bridger"><meta name=twitter:title content="Solving Machine Learning Performance Anti-Patterns: a Systematic Approach"><meta name=twitter:image content="https://paulbridger.com/posts/nsight-systems-systematic-optimization/images/pattern-overview.png"></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a href=/><span>paulbridger.com</span></a></h2><ul><li><a href=https://paulbridger.com/posts/nsight-systems-systematic-optimization/ class=active>Solving Machine Learning Performance Anti-Patterns: a Systematic Approach</a></li><li><a href=https://paulbridger.com/posts/video-analytics-pytorch-pipeline/>A Simple and Flexible Pytorch Video Pipeline</a></li><li><a href=https://paulbridger.com/posts/video-analytics-pipeline-tuning/>Object Detection from 9 FPS to 650 FPS in 6 Steps</a></li><li><a href=https://paulbridger.com/posts/video-analytics-deepstream-pipeline/>Object Detection at 1840 FPS with TorchScript, TensorRT and DeepStream</a></li><li><a href=https://paulbridger.com/posts/tensorrt-object-detection-quantized/>Object Detection at 2530 FPS with TensorRT and 8-Bit Quantization</a></li><li><a href=https://paulbridger.com/posts/mastering-torchscript/>Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification</a></li><li><a href=https://paulbridger.com/posts/consulting/>Consulting / Hire Me</a></li><li><a href=https://paulbridger.com/posts/about/>About Paul Bridger</a></li></ul></nav><script>(function(){var a=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Solving Machine Learning Performance Anti-Patterns: a Systematic Approach</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><div class=book-toc-div><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#finding-the-patterns>Finding the Patterns</a><ul><li><a href=#tracing-with-nsight-systems>Tracing with Nsight Systems</a></li><li><a href=#nvtx-mapping-logic-to-metrics>NVTX: Mapping Logic to Metrics</a></li></ul></li><li><a href=#understanding-the-patterns>Understanding the Patterns</a><ul><li><a href=#hostdevice-system-concept>Host/Device System Concept</a></li></ul></li><li><a href=#ml-optimization-pattern-language>ML Optimization Pattern Language</a><ul><li><a href=#pattern-gpu-compute-bound>Pattern: GPU Compute Bound</a></li><li><a href=#pattern-cpu-cuda-api-bound>Pattern: CPU CUDA API Bound</a></li><li><a href=#pattern-synchronization-bound>Pattern: Synchronization Bound</a></li><li><a href=#pattern-cpu-compute-bound>Pattern: CPU Compute Bound</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#join-me-at-womboai->Join Me at WOMBO.ai ðŸŒˆ</a></li></ul></nav><nav>Have a question or comment?<br><a href="https://news.ycombinator.com/item?id=27663309">Join the discussion on Hacker News <img src=/images/y.svg></a></nav></div></aside></header><article class=markdown><h1><a href=/posts/nsight-systems-systematic-optimization/>Solving Machine Learning Performance Anti-Patterns: a Systematic Approach</a></h1><h5>June 24, 2021</h5><div><a href=/categories/machine-learning-productionization/>Machine Learning Productionization</a></div><div><a href=/tags/nsight-systems/>Nsight Systems</a>,
<a href=/tags/nvtx/>NVTX</a>,
<a href=/tags/optimization/>Optimization</a>,
<a href=/tags/tensorrt/>TensorRT</a>,
<a href=/tags/torchscript/>TorchScript</a>,
<a href=/tags/quantization/>Quantization</a></div><h2 id=intro>Intro
<a class=anchor href=#intro>#</a></h2><p>This article is a high-level introduction to an efficient worfklow for optimizing runtime performance of machine learning systems running on the GPU. This approach is not revolutionary â€” we simply find the biggest problems and then fix them, again and again â€” but without the right tools and knowledge it&rsquo;s hard to be disciplined and efficient in this cycle.</p><p>Without knowing exactly why a system is slow there is a temptation to use the &ldquo;it could be this&rdquo; approach â€” successively guessing at interventions which might fix the problem. Guessing is usually a waste of time when it comes to any optimization work. NVIDIA&rsquo;s Nsight Systems helps us quickly understand system performance, and it&rsquo;s one of the main tools I use when tuning ML systems for production.</p><p>Once we have identified the main performance problems, being able to solve these is a matter of understanding the system (software and hardware) deeply enough and using the right tool for the job. This article will explain the techniques and tools I use â€” when to apply them and why they work.</p><p>I&rsquo;ll introduce a set of common utilization patterns and outline effective approaches to improve performance. We&rsquo;ll dig into these patterns later on:</p><p><img src=/posts/nsight-systems-systematic-optimization/images/pattern-overview.svg alt="Pattern Overview"></p><p>In my consulting work I&rsquo;ve seen many startups trying to take research models into production and facing major performance or cost hurdles. Using this systematic approach I&rsquo;ve achieved throughput improvements of up to 15X with just a few days of intense work. A systematic approach delivers the biggest improvements first, and is the most efficient path to delivering well-tuned systems.</p><h2 id=finding-the-patterns>Finding the Patterns
<a class=anchor href=#finding-the-patterns>#</a></h2><h3 id=tracing-with-nsight-systems>Tracing with Nsight Systems
<a class=anchor href=#tracing-with-nsight-systems>#</a></h3><p>NVIDIA&rsquo;s Nsight Systems is easy to get started with and delivers a rich trace of key resources and events. If you learn one tool for understanding the performance of machine learning systems, this should be it.</p><p>We can begin tracing without any changes to application code â€” as simple as this:</p><pre><code>$ nsys profile -t cuda,cudnn,nvtx,osrt -o &lt;report_path&gt; &lt;process_command_line&gt;
</code></pre><p>This will produce a report at <code>&lt;report_path></code> which can be loaded and analyzed using the Nsight Systems desktop client. Here&rsquo;s an example:</p><a href=/posts/nsight-systems-systematic-optimization/images/nsight-intro.png><figure style="margin:2rem 0"><img style=max-width:100%;width:auto;height:auto src=/posts/nsight-systems-systematic-optimization/images/nsight-intro_hu3cda51f7b976ecb143f0d2f57efa06db_611868_1280x750_fill_box_top_2.png width=1280 height=750><figcaption><small></small></figcaption></figure></a><p>Looking at this sample Nsight Systems report, we can see several important features:</p><ul><li>Traces of aggregated resource consumption over time (GPU and CPU utilization) tell us generally where to focus.</li><li>Ranges in several tracks corresponding to on-device kernel executions, memory transfers, synchronization activity, and host CUDA API calls.</li><li>Activity is split into traces for any GPUs, and also for the host where it is subdivided by threads.</li></ul><h3 id=nvtx-mapping-logic-to-metrics>NVTX: Mapping Logic to Metrics
<a class=anchor href=#nvtx-mapping-logic-to-metrics>#</a></h3><p>NVTX is an indispensible toolkit that hugely increases the power of Nsight Systems reports for optimization. Using NVTX we can easily delineate logical sections of model code and these appear as highly precise ranges in Nsight Systems traces. Viewing resource consumption metrics and low-level operations (CUDA kernels, memory transfers, synchronization points) alongside your logical ranges allows us to deeply understand execution performance.</p><a href=/posts/nsight-systems-systematic-optimization/images/nsight-nvtx.png><figure style="margin:2rem 0"><img style=max-width:100%;width:auto;height:auto src=/posts/nsight-systems-systematic-optimization/images/nsight-nvtx_hu3cda51f7b976ecb143f0d2f57efa06db_454158_1280x750_fill_box_top_2.png width=1280 height=750><figcaption><small></small></figcaption></figure></a><blockquote class="book-hint info"><p><strong>NVTX Ranges on Host and Device May Not Match, but Both Are Correct</strong></p><p>The host submits work via a command queue to the GPU for execution. Due to this asynchronous relationship the start of an NVTX range in the host process will often be well before the start of that range on the GPU.</p></blockquote><p>NVTX can be accessed as a C or Python API. I use the following context manager which hooks into the Pytorch NVTX bindings:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=nd>@contextlib.contextmanager</span>
<span class=k>def</span> <span class=nf>nvtx_range</span><span class=p>(</span><span class=n>msg</span><span class=p>):</span>
    <span class=n>depth</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>nvtx</span><span class=o>.</span><span class=n>range_push</span><span class=p>(</span><span class=n>msg</span><span class=p>)</span>
    <span class=k>try</span><span class=p>:</span>
        <span class=k>yield</span> <span class=n>depth</span>
    <span class=k>finally</span><span class=p>:</span>
        <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>nvtx</span><span class=o>.</span><span class=n>range_pop</span><span class=p>()</span>
</code></pre></div><p>Here&rsquo;s an example of usage:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>Net</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
        <span class=k>with</span> <span class=n>nvtx_range</span><span class=p>(</span><span class=s1>&#39;Net.forward&#39;</span><span class=p>):</span>
            <span class=k>with</span> <span class=n>nvtx_range</span><span class=p>(</span><span class=s1>&#39;normalize&#39;</span><span class=p>):</span>
                <span class=n>X</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>half</span><span class=p>()</span><span class=o>.</span><span class=n>divide</span><span class=p>(</span><span class=mi>255</span><span class=p>)</span>
            <span class=k>with</span> <span class=n>nvtx_range</span><span class=p>(</span><span class=s1>&#39;blur&#39;</span><span class=p>):</span>
                <span class=k>return</span> <span class=n>gaussian_blur</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>sigma</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</code></pre></div><h2 id=understanding-the-patterns>Understanding the Patterns
<a class=anchor href=#understanding-the-patterns>#</a></h2><p>In order to translate the information in an Nsight Systems report into actual insight, an understanding of how the host (CPU) and device (GPU/TPU) work together is essential.</p><h3 id=hostdevice-system-concept>Host/Device System Concept
<a class=anchor href=#hostdevice-system-concept>#</a></h3><p>All models are wrong, but some are useful â€” we need a simple abstraction of the host/device relationship so we can reason about performance and understand the patterns we see in Nsight traces.</p><p>The devices (GPUs) in a system behave like very powerful asynchronous coprocessors. The host (CPU) submits fully-specified chunks of work into a queue and the device performs these operations asynchronously and independently, only returning results to the host when requested via a memory transfer.</p><p>At this level of abstraction, an example trace might look like this:</p><p><img src=/posts/nsight-systems-systematic-optimization/images/host-device-concept.svg alt="Host/Device Concept"></p><p>Given this asynchronous relationship, the significant latency in host/device communication, and the fact that the device is far more powerful than the host in raw compute terms, some guidelines become clear:</p><ul><li>Do as much work on the device as possible.</li><li>Work hard to remove synchronization points between host and device, as these can leave either side idle.</li><li>If possible, reduce communication (API calls and memory transfers) between host and device by batching or combining operations.</li></ul><p>These principles are in rough order of priority, and like all guidelines there are times they should be broken.</p><p>Next we&rsquo;ll take a tour through some major patterns of suboptimal performance â€” many of which map directly to violations of these principles.</p><h2 id=ml-optimization-pattern-language>ML Optimization Pattern Language
<a class=anchor href=#ml-optimization-pattern-language>#</a></h2><blockquote class="book-hint info">A catalog of common patterns and sample solutions â€” I can call that a pattern language, right? If not, here&rsquo;s a link to <a href="https://news.ycombinator.com/item?id=27663309">a Hacker News thread</a> where you can call me pretentious.</blockquote><p>Machine learning systems show distinct patterns of resource consumption, and each of these patterns requires a different approach to improving performance.</p><p>Real-world systems usually exhibit several different patterns in different parts of the inference pipeline so quite often we&rsquo;ll need to apply multiple of the approaches below. For example, post-processing logic is highly prone to being CPU compute bound or synchronization bound, whereas the backbone of vision models are often GPU compute bound.</p><p>Below I&rsquo;ll show the most common trace patterns in my experience tuning inference pipelines and detail some approaches to optimization in each case.</p><h3 id=pattern-gpu-compute-bound>Pattern: GPU Compute Bound
<a class=anchor href=#pattern-gpu-compute-bound>#</a></h3><p>In this pattern, the GPU is continually running CUDA kernels corresponding to tensor transformations, activation functions, weight multiplications etc. These operations are being enqueued by the host faster than the GPU can execute them, so the GPU is able to stay at high utilization. In this state, a less fine-grained tool like <code>nvidia-smi</code> would show >90% GPU utilization.</p><p><img src=/posts/nsight-systems-systematic-optimization/images/gpu-compute-bound.svg alt="GPU Compute Bound"></p><h4 id=nsight-systems-example>Nsight Systems Example
<a class=anchor href=#nsight-systems-example>#</a></h4><a href=/posts/nsight-systems-systematic-optimization/images/nsight-gpu-compute-bound.png><figure style="margin:2rem 0"><img style=max-width:100%;width:auto;height:auto src=/posts/nsight-systems-systematic-optimization/images/nsight-gpu-compute-bound_hu737b9166b07a8a13c355dc78ae6c1858_513452_1280x770_fill_box_top_2.png width=1280 height=770><figcaption><small></small></figcaption></figure></a><h4 id=distinguishing-features>Distinguishing Features
<a class=anchor href=#distinguishing-features>#</a></h4><ul><li>GPU utilization resource trace shows >90%, without gaps, and kernels are executed sequentially without delays.</li><li>GPU memory transfers are absent or rare, especially DtoH or HtoD transfers.</li><li>Host CUDA API trace shows kernels being submitted faster than the GPU completes them, and includes waiting time for synchronization.</li></ul><h4 id=improving-gpu-compute-performance>Improving GPU Compute Performance
<a class=anchor href=#improving-gpu-compute-performance>#</a></h4><p>In the scope of machine learning systems, this pattern is a good one to see. It means you are using your most expensive hardware with high utilization and you are avoiding the very common pitfall of mixing CPU and GPU computation together. However, if a large portion of your run time is GPU compute bound then this represents a good opportunity for optimization.</p><h4 id=optimization-approaches>Optimization approaches
<a class=anchor href=#optimization-approaches>#</a></h4><ul><li>Use a lower-precision version of your model: float16 or even int8 quantization can give up to 2x or 4x improvements in throughput.</li><li>Use a profiling-based model compiler like TensorRT: this will fuse layers, use optimized kernels, and determine optimal kernel launch parameters for your specific hardware.</li><li>Perform model-pruning using a framework like the Transfer Learning Toolkit (TLT): reduce the number of computations performed with a small impact to accuracy.</li><li>Use more powerful GPUs: spending more money is usually not the goal, however this is the only trace pattern where upgrading GPUs is a good choice.</li></ul><h3 id=pattern-cpu-cuda-api-bound>Pattern: CPU CUDA API Bound
<a class=anchor href=#pattern-cpu-cuda-api-bound>#</a></h3><p>In terms of raw compute GPUs are surprisingly powerful and it is easy to get into a situation where your host/CPU is submitting work to the GPU as fast as possible but still cannot keep up. The CPU is constantly sending work into a non-blocking command queue, but the GPU finishes the work in less time than it takes to configure and enqueue. Unsurprisingly, this is much more likely to be a problem with a Python host process.</p><p><img src=/posts/nsight-systems-systematic-optimization/images/cpu-cuda-api-bound.svg alt="CPU CUDA API Bound"></p><h4 id=nsight-systems-example-1>Nsight Systems Example
<a class=anchor href=#nsight-systems-example-1>#</a></h4><a href=/posts/nsight-systems-systematic-optimization/images/nsight-cpu-cuda-api-bound.png><figure style="margin:2rem 0"><img style=max-width:100%;width:auto;height:auto src=/posts/nsight-systems-systematic-optimization/images/nsight-cpu-cuda-api-bound_hu570347f81411de03383c6a3d73f9a4b0_445793_1280x770_fill_box_top_2.png width=1280 height=770><figcaption><small></small></figcaption></figure></a><h4 id=distinguishing-features-1>Distinguishing features
<a class=anchor href=#distinguishing-features-1>#</a></h4><ul><li>CPU utilization is >90% and the host is continuously making CUDA API calls.</li><li>GPU utilization is &lt;80%, and contains frequent gaps.</li><li>Memory transfers between host and device do not dominate the trace.</li></ul><h4 id=improving-cuda-api-performance>Improving CUDA API Performance
<a class=anchor href=#improving-cuda-api-performance>#</a></h4><p>Improving performance when you are spending all your host time on submitting tensor operations can be very easy or very challenging, depending on whether your existing logic incorporates a batch dimension. If all processing is expressed in terms of a batch then increasing this batch dimension is a great way to shift the bottleneck from CPU to GPU.</p><p>If your current logic is expressed per item (frame by frame, for example) then you&rsquo;ll probably need to think through every line of the code as you implement batching. On the positive side, the results of this kind of transformation can be very impressive and with the GPU now a bottleneck many further optimizations have been unlocked.</p><h4 id=optimization-approaches-1>Optimization approaches
<a class=anchor href=#optimization-approaches-1>#</a></h4><ul><li>Increase batch sizes, sending more work to the GPU per API call.</li><li>Export and run this logic with TorchScript, resulting in much faster submission of operations.</li><li>Rewrite tensor operations to do more work with fewer API calls.</li><li>Reimplement your logic in C/C++.</li></ul><h3 id=pattern-synchronization-bound>Pattern: Synchronization Bound
<a class=anchor href=#pattern-synchronization-bound>#</a></h3><p>This pattern commonly occurs in conjunction with CUDA API bound code, often during post-processing where small GPU operations are mixed with dependent CPU operations. This alternating GPU/CPU computation usually requires many small memory transfers and associated syncronization points, and you pay a latency cost each time. If you are writing post-processing logic on the GPU but find yourself writing loops in host code, then you may be creating this pattern.</p><p><img src=/posts/nsight-systems-systematic-optimization/images/synchronization-bound.svg alt="Synchronization Bound"></p><h4 id=nsight-systems-example-2>Nsight Systems Example
<a class=anchor href=#nsight-systems-example-2>#</a></h4><a href=/posts/nsight-systems-systematic-optimization/images/nsight-synchronization-bound.png><figure style="margin:2rem 0"><img style=max-width:100%;width:auto;height:auto src=/posts/nsight-systems-systematic-optimization/images/nsight-synchronization-bound_hue7269763c3fb52323d4ba7ae63d2d3ae_449710_1280x750_fill_box_top_2.png width=1280 height=750><figcaption><small></small></figcaption></figure></a><p>In this heavily zoomed-in trace, note all the green synchronization points in the CUDA API trace, and all the corresponding tiny DtoH (device to host) memory transfers. This is a classic issue with post-processing logic.</p><h4 id=distinguishing-features-2>Distinguishing features
<a class=anchor href=#distinguishing-features-2>#</a></h4><ul><li>CPU utilization is moderate.</li><li>GPU utilization is moderate.</li><li>Synchronization points dominate the trace, usually associated with small memory transfers.</li></ul><h4 id=reducing-synchronization>Reducing Synchronization
<a class=anchor href=#reducing-synchronization>#</a></h4><p>Synchronization is required when a result from either host or device is needed on the other, so the key to solving this pattern is to fix interleaved host/device computation.</p><p>Sometimes this can be done by turning loops into vectorized tensor ops on the GPU, sometimes splitting out the algorithm into batched GPU computation and subsequent looping host computation can help, and sometimes moving everything to the host comes out ahead. A low-effort fix is to increase batch sizes which proportionally reduces interleaved computation, but doesn&rsquo;t fix the underlying problem.</p><h4 id=optimization-approaches-2>Optimization approaches
<a class=anchor href=#optimization-approaches-2>#</a></h4><ul><li>Convert as many operations as possible to execute on the GPU.</li><li>Deinterleave GPU/CPU processing, computing large batches on GPU then consuming these as required in the host process.</li><li>Alternatively, in rare cases converting all operations to execute on CPU may increase performance.</li><li>Increase batch sizes, allowing more work to happen on the GPU between synchronization points.</li></ul><h3 id=pattern-cpu-compute-bound>Pattern: CPU Compute Bound
<a class=anchor href=#pattern-cpu-compute-bound>#</a></h3><p>If your machine learning process is CPU compute bound then you may have a simple bug where some portion of your model code is running on CPU rather than GPU.</p><p>Legacy machine learning systems used to do preprocessing and postprocessing using CPU only â€” only using the GPU for the core model. We now have hardware accelerated augmentation libraries and flexible on-GPU transformation pipelines, so for either training or production inference doing substantial preprocessing or postprocessing on CPU rarely makes sense.</p><p>If you have this pattern and it was an honest mistake, I forgive you.</p><p><img src=/posts/nsight-systems-systematic-optimization/images/cpu-compute-bound.svg alt="CPU Compute Bound"></p><h4 id=nsight-systems-example-3>Nsight Systems Example
<a class=anchor href=#nsight-systems-example-3>#</a></h4><a href=/posts/nsight-systems-systematic-optimization/images/nsight-cpu-compute-bound.png><figure style="margin:2rem 0"><img style=max-width:100%;width:auto;height:auto src=/posts/nsight-systems-systematic-optimization/images/nsight-cpu-compute-bound_hu5647ace45ab9b7a1e56e4454451750b2_477583_1280x750_fill_box_top_2.png width=1280 height=750><figcaption><small></small></figcaption></figure></a><p>The Nsight Systems trace above looks horrific.</p><h4 id=distinguishing-features-3>Distinguishing features
<a class=anchor href=#distinguishing-features-3>#</a></h4><ul><li>CPU utilization is >90% and few CUDA API calls are being made.</li><li>GPU is mostly idle, with no or few memory transfers.</li></ul><h4 id=optimization-approaches-3>Optimization approaches
<a class=anchor href=#optimization-approaches-3>#</a></h4><ul><li>Move the compute to GPU by expressing the algorithm as highly vectorized tensor operations.</li><li>If work cannot be moved to the GPU, do this work asynchronously with respect to tasks which can use the GPU.</li></ul><h2 id=conclusion>Conclusion
<a class=anchor href=#conclusion>#</a></h2><p>The most important idea in this post is that of being systematic when optimizing performance â€” measuring and observing well-defined problems before you spend time fixing them. This idea is well known but not well followed (you are fine, I am talking about other people).</p><p>With a familiar tracing tool close to hand, it becomes easy and natural to focus on the biggest opportunities and avoid wasting time on problems that don&rsquo;t exist. Nsight Systems is in a sweet spot of being easy to use, highly detailed and accurate, and straightforward to analyze once you know what you&rsquo;re looking for.</p><p>The utilization patterns described above, together with the suggested optimization approaches, are the highest-level distillation possible of my experience tuning production machine learning systems. I hope you find them useful!</p><h2 id=join-me-at-womboai->Join Me at WOMBO.ai ðŸŒˆ
<a class=anchor href=#join-me-at-womboai->#</a></h2><p>I don&rsquo;t have a soundcloud, but if you liked this article you&rsquo;re probably the kind of person we&rsquo;re looking to work with at WOMBO.ai.</p><p><a href=https://www.wombo.ai>WOMBO.ai</a> is Canada&rsquo;s fastest growing app (ever), a true rocketship with over 50M installs in the last three months and closing in on a billion lipsync videos created. We&rsquo;re looking for early employees and leaders across a bunch of different roles: check <a href=https://www.wombo.ai/careers>the careers page</a>!</p><p>The team has an expansive vision of what is possible with cutting edge ML and synthetic media, and the funding to make this happen. If you want to work on SoTA generative ML or push the boundaries of production inference efficiency, then come work with me at WOMBO. :)</p></article><article class="markdown social"><hr><section class=social-share><ul class=share-icons><li><a href="https://twitter.com/intent/follow?ref_src=twsrc%5Etfw&region=follow_link&screen_name=paul_bridger&tw_p=followbutton" target=_blank rel=noopener aria-label="Follow on Twitter" class="share-btn twitter"><svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path fill="#fff" d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5.0-78.8 35.3-78.8 78.8.0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3C20 26 16.1 39.6 16.1 54c0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1.0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4.0-12.6-.4-18.8-1.1C34.9 299 76.3 312 120.8 312c144.9.0 224.1-120 224.1-224.1.0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg><p>Twitter</p></a></li><li class=share-label><a href="https://twitter.com/intent/follow?ref_src=twsrc%5Etfw&region=follow_link&screen_name=paul_bridger&tw_p=followbutton" target=_blank rel=noopener aria-label="Follow on Twitter">Follow on Twitter</a></li></ul><ul class=share-icons><li><a href=https://www.linkedin.com/in/paulbridger/ target=_blank rel=noopener aria-label="Connect on LinkedIn" class="share-btn linkedin"><svg class="widget-social__link-icon icon icon-linkedin" width="24" height="24" viewBox="0 0 352 352"><path fill="#fff" d="M0 40v272c0 21.9 18.1 40 40 40h272c21.9.0 40-18.1 40-40V40c0-21.9-18.1-40-40-40H40C18.1.0.0 18.1.0 40zm312-8c4.6.0 8 3.4 8 8v272c0 4.6-3.4 8-8 8H40c-4.6.0-8-3.4-8-8V40c0-4.6 3.4-8 8-8H312zM59.5 87c0 15.2 12.3 27.5 27.5 27.5s27.5-12.3 27.5-27.5S102.2 59.5 87 59.5 59.5 71.8 59.5 87zM187 157h-1v-21h-45v152h47v-75c0-19.8 3.9-39 28.5-39 24.2.0 24.5 22.4 24.5 40v74h47v-83.5c0-40.9-8.7-72-56.5-72-23 0-38.2 12.6-44.5 24.5zM64 288h47.5V136H64V288z"/></svg><p>LinkedIn</p></a></li><li class=share-label><a href=https://www.linkedin.com/in/paulbridger/ target=_blank rel=noopener aria-label="Connect on LinkedIn">Connect on LinkedIn</a></li></ul><ul class=share-icons><li><a href=mailto:paul@paulbridger.com target=_blank class="share-btn email"><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path fill="#fff" d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><p>Email</p></a></li><li class=share-label><a href=mailto:paul@paulbridger.com target=_blank>Contact by Email</a></li></ul><ul class=share-icons><li><a href="https://news.ycombinator.com/item?id=27663309" class="share-btn yc"><svg class="widget-social__link-icon icon icon-yc" width="24" height="24" viewBox="3 3 24 24"><path fill="#fff" d="M14 17 8.8 7.3h2.4l3 6.1c0 .1.1.2.2.3.1.1.1.2.2.4l.1.1s0 .1.0.1c.1.2.1.3.2.5.1.1.1.3.2.4.1-.3.3-.5.4-.9.1-.3.3-.6.5-.9l3-6.1h2.2L16 17.1v6.2h-2V17z"/></svg><p>Hacker News</p></a></li><li class=share-label><a href="https://news.ycombinator.com/item?id=27663309" target=_blank>Discuss on HN</a></li></ul></section></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function a(c){const a=window.getSelection(),b=document.createRange();b.selectNodeContents(c),a.removeAllRanges(),a.addRange(b)}document.querySelectorAll("pre code").forEach(b=>{b.addEventListener("click",function(c){a(b.parentElement),navigator.clipboard&&navigator.clipboard.writeText(b.parentElement.textContent)})})})()</script><hr><b>&copy; Paul Bridger 2020</b></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><div class=book-toc-div><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#finding-the-patterns>Finding the Patterns</a><ul><li><a href=#tracing-with-nsight-systems>Tracing with Nsight Systems</a></li><li><a href=#nvtx-mapping-logic-to-metrics>NVTX: Mapping Logic to Metrics</a></li></ul></li><li><a href=#understanding-the-patterns>Understanding the Patterns</a><ul><li><a href=#hostdevice-system-concept>Host/Device System Concept</a></li></ul></li><li><a href=#ml-optimization-pattern-language>ML Optimization Pattern Language</a><ul><li><a href=#pattern-gpu-compute-bound>Pattern: GPU Compute Bound</a></li><li><a href=#pattern-cpu-cuda-api-bound>Pattern: CPU CUDA API Bound</a></li><li><a href=#pattern-synchronization-bound>Pattern: Synchronization Bound</a></li><li><a href=#pattern-cpu-compute-bound>Pattern: CPU Compute Bound</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#join-me-at-womboai->Join Me at WOMBO.ai ðŸŒˆ</a></li></ul></nav><nav>Have a question or comment?<br><a href="https://news.ycombinator.com/item?id=27663309">Join the discussion on Hacker News <img src=/images/y.svg></a></nav></div></div></aside></main></body></html>