<!doctype html><html lang=en dir=ltr>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="In this article we take performance of the SSD300 model even further, leaving Python behind and moving towards true production deployment technologies: TorchScript, TensorRT and DeepStream. We also identify and understand several limitations in Nvidia&rsquo;s DeepStream framework, and then remove them by modifying how the nvinfer element works.">
<meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Object Detection at 1840 FPS with TorchScript, TensorRT and DeepStream">
<meta property="og:description" content="In this article we take performance of the SSD300 model even further, leaving Python behind and moving towards true production deployment technologies: TorchScript, TensorRT and DeepStream. We also identify and understand several limitations in Nvidia&rsquo;s DeepStream framework, and then remove them by modifying how the nvinfer element works.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-10-17T08:43:23+02:00">
<meta property="article:modified_time" content="2020-10-17T08:43:23+02:00">
<title>Object Detection at 1840 FPS with TorchScript, TensorRT and DeepStream | paulbridger.com</title>
<link rel=manifest href=/manifest.json>
<link rel=icon href=/favicon.png type=image/x-icon>
<link rel=stylesheet href=/book.min.b5056d4fbae4a365a3e7a6f0af8a29daf564e71fa285cacf5f41e0c78741630d.css integrity="sha256-tQVtT7rko2Wj56bwr4op2vVk5x+ihcrPX0Hgx4dBYw0=" crossorigin=anonymous>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-3441595-4','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<meta name=twitter:card content="summary">
<meta name=twitter:site content="@paul_bridger">
<meta name=twitter:title content="Object Detection at 1840 FPS with TorchScript, TensorRT and DeepStream">
<meta name=twitter:image content="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/images/ds_80pc_tensorrt_hacked_ds_two_batch.png">
<meta property="og:image" content="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/images/ds_80pc_tensorrt_hacked_ds_two_batch.png">
</head>
<body dir=ltr>
<input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control>
<main class="container flex">
<aside class=book-menu>
<div class=book-menu-content>
<nav>
<h2 class=book-brand>
<a href=/><span>paulbridger.com</span>
</a>
</h2>
<ul>
<li>
<a href=https://paulbridger.com/posts/pytorch-memory-tuning/>PyTorch Memory Tuning</a>
</li>
<li>
<a href=https://paulbridger.com/posts/pytorch-tuning-tips/>PyTorch Performance Features and How They Interact</a>
</li>
<li>
<a href=https://paulbridger.com/posts/nsight-systems-systematic-optimization/>Solving Machine Learning Performance Anti-Patterns: a Systematic Approach</a>
</li>
<li>
<a href=https://paulbridger.com/posts/video-analytics-pytorch-pipeline/>A Simple and Flexible Pytorch Video Pipeline</a>
</li>
<li>
<a href=https://paulbridger.com/posts/video-analytics-pipeline-tuning/>Object Detection from 9 FPS to 650 FPS in 6 Steps</a>
</li>
<li>
<a href=https://paulbridger.com/posts/video-analytics-deepstream-pipeline/ class=active>Object Detection at 1840 FPS with TorchScript, TensorRT and DeepStream</a>
</li>
<li>
<a href=https://paulbridger.com/posts/tensorrt-object-detection-quantized/>Object Detection at 2530 FPS with TensorRT and 8-Bit Quantization</a>
</li>
<li>
<a href=https://paulbridger.com/posts/mastering-torchscript/>Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification</a>
</li>
<li>
<a href=https://paulbridger.com/posts/consulting/>Consulting / Hire Me</a>
</li>
<li>
<a href=https://paulbridger.com/posts/about/>About Paul Bridger</a>
</li>
</ul>
</nav>
<script>(function(){var a=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>
</div>
</aside>
<div class=book-page>
<header class=book-header>
<div class="flex align-center justify-between">
<label for=menu-control>
<img src=/svg/menu.svg class=book-icon alt=Menu>
</label>
<strong>Object Detection at 1840 FPS with TorchScript, TensorRT and DeepStream</strong>
<label for=toc-control>
<img src=/svg/toc.svg class=book-icon alt="Table of Contents">
</label>
</div>
<aside class="hidden clearfix">
<div class=book-toc-div>
<nav id=TableOfContents>
<ul>
<li><a href=#intro>Intro</a>
<ul>
<li><a href=#torchscript-vs-tensorrt>ðŸ”¥TorchScript vs TensorRTðŸ”¥</a></li>
</ul>
</li>
<li><a href=#stage-0-python-baseline>Stage 0: Python Baseline</a></li>
<li><a href=#stage-1-normal-deepstream-mdash-100-torchscript>Stage 1: Normal DeepStream â€” 100% TorchScript</a>
<ul>
<li><a href=#hybrid-deepstream-pipeline>Hybrid DeepStream Pipeline</a></li>
</ul>
</li>
<li><a href=#stage-2-hacked-deepstream-mdash-100-torchscript>Stage 2: Hacked DeepStream â€” 100% TorchScript</a></li>
<li><a href=#stage-3-hacked-deepstream-mdash-80-tensorrt-20-torchscript>Stage 3: Hacked DeepStream â€” 80% TensorRT, 20% TorchScript</a></li>
<li><a href=#stage-4-normal-deepstream-mdash-80-tensorrt-20-torchscript>Stage 4: Normal DeepStream â€” 80% TensorRT, 20% TorchScript</a></li>
<li><a href=#stage-5-horizontal-scalability>Stage 5: Horizontal Scalability</a></li>
<li><a href=#conclusion>Conclusion</a>
<ul>
<li><a href=#caveats-limitations-and-excuses>Caveats, Limitations and Excuses</a></li>
</ul>
</li>
</ul>
</nav>
<nav>
Have a question or comment?<br>
<a href="https://news.ycombinator.com/item?id=24817173">Join the discussion on Hacker News <img src=/images/y.svg></a>
</nav>
</div>
</aside>
</header>
<article class=markdown>
<h1>
<a href=/posts/video-analytics-deepstream-pipeline/>Object Detection at 1840 FPS with TorchScript, TensorRT and DeepStream</a>
</h1>
<h5>October 17, 2020</h5>
<div>
<a href=/categories/visual-analytics/>Visual Analytics</a>,
<a href=/categories/machine-learning-productionization/>Machine Learning Productionization</a>
</div>
<div>
<a href=/tags/ssd300/>SSD300</a>,
<a href=/tags/pytorch/>Pytorch</a>,
<a href=/tags/object-detection/>Object Detection</a>,
<a href=/tags/optimization/>Optimization</a>,
<a href=/tags/deepstream/>DeepStream</a>,
<a href=/tags/torchscript/>TorchScript</a>,
<a href=/tags/tensorrt/>TensorRT</a>,
<a href=/tags/onnx/>ONNX</a>,
<a href=/tags/nvtx/>NVTX</a>,
<a href=/tags/nsight-systems/>Nsight Systems</a>
</div>
<h2 id=intro>
Intro
<a class=anchor href=#intro>#</a>
</h2>
<p>Previously, we took a <a href=/posts/video-analytics-pytorch-pipeline/>simple video pipeline</a> and made it as fast as we could without sacrificing the flexibility of the Python runtime. It&rsquo;s amazing how far you can go â€” <a href=/posts/video-analytics-pipeline-tuning/>9 FPS to 650 FPS</a> â€” but we did not reach full hardware utilization and the pipeline did not scale linearly beyond a single GPU. There is evidence (measured using <a href=https://github.com/chrisjbillington/gil_load>gil_load</a>) that we were throttled by a fundamental Python limitation with multiple threads fighting over the <a href=https://wiki.python.org/moin/GlobalInterpreterLock>Global Interpreter Lock</a> (GIL).</p>
<p>In this article we&rsquo;ll take performance of the same <a href=https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/>SSD300 model</a> even further, leaving Python behind and moving towards true production deployment technologies:</p>
<ul>
<li>
<p><a href=https://pytorch.org/docs/stable/jit.html><strong>TorchScript.</strong></a> Instead of running directly in the Pytorch runtime, we&rsquo;ll export our model using TorchScript tracing into a form that can be executed portably using the <code>libtorch</code> C++ runtime.</p>
</li>
<li>
<p><a href=https://developer.nvidia.com/tensorrt><strong>TensorRT.</strong></a> This toolset from Nvidia includes a &ldquo;deep learning inference optimizer&rdquo; â€” a compiler for optimizing CUDA-based computational graphs. We&rsquo;ll use this to squeeze out every drop of inference efficiency.</p>
</li>
<li>
<p><a href=https://developer.nvidia.com/deepstream-sdk><strong>DeepStream.</strong></a> While <a href=https://gstreamer.freedesktop.org/>Gstreamer</a> gives us an extensive library of elements to build media pipelines with, DeepStream expands this library with a set of GPU-accelerated elements specialized for machine learning.</p>
</li>
</ul>
<p>These technologies fit together like this:</p>
<p><img src=/posts/video-analytics-deepstream-pipeline/images/deepstream_hybrid.svg alt="DeepStream Hybrid Architecture"></p>
<p>This article will not be a step-by-step tutorial with code examples, but will show what is possible when these technologies are combined. The associated repository is here: <a href=https://github.com/pbridger/deepstream-video-pipeline>github.com/pbridger/deepstream-video-pipeline</a>.</p>
<h3 id=torchscript-vs-tensorrt>
ðŸ”¥TorchScript vs TensorRTðŸ”¥
<a class=anchor href=#torchscript-vs-tensorrt>#</a>
</h3>
<p>Both TorchScript and TensorRT can produce a deployment-ready form of our model, so why do we need both? These great tools may eventually be competitors but in 2020 they are complementary â€” they each have weaknesses that are compensated for by the other.</p>
<p><strong>TorchScript.</strong> With a few lines of <code>torch.jit</code> code we can generate a deployment-ready asset from essentially any Pytorch model that will run anywhere libtorch runs. It&rsquo;s not inherently faster (it is submitting approximately the same sequence of kernels) but the libtorch runtime will perform better under high concurrency. However, without care TorchScript output may have performance and portability surprises (I&rsquo;ll cover some of these in a later article).</p>
<p><strong>TensorRT.</strong> An unparalleled model compiler for Nvidia hardware, but for Pytorch or <a href=https://onnx.ai/>ONNX</a>-based models it has incomplete support and suffers from poor portability. There is a plugin system to add arbitrary layers and postprocessing, but this low-level work is out of reach for groups without specialized deployment teams. TensorRT also doesn&rsquo;t support cross-compilation so models must be optimized directly on the target hardware â€” not great for embedded platforms or highly diverse compute ecosystems.</p>
<p>Let&rsquo;s begin with a baseline from the previous post in this series â€” <a href=/posts/video-analytics-pipeline-tuning/>Object Detection from 9 FPS to 650 FPS in 6 Steps</a>.</p>
<h2 id=stage-0-python-baseline>
Stage 0: Python Baseline
<a class=anchor href=#stage-0-python-baseline>#</a>
</h2>
<table>
<thead>
<tr>
<th style=text-align:left>Code</th>
<th style=text-align:left>Nsight Systems Trace</th>
<th style=text-align:left>Gstreamer Pipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left><a href=https://github.com/pbridger/pytorch-video-pipeline/blob/master/tuning_postprocess_2.py>tuning_postprocess_2.py</a></td>
<td style=text-align:left><a href=/posts/video-analytics-pipeline-tuning/logs/tuning_postprocess_2.qdrep>tuning_postprocess_2.qdrep</a></td>
<td style=text-align:left><a href=/posts/video-analytics-pipeline-tuning/logs/tuning_postprocess_2.pipeline.dot.png>tuning_postprocess_2.pipeline.dot.png</a></td>
</tr>
</tbody>
</table>
<p>The <a href=/posts/video-analytics-pipeline-tuning/#stage-2-postprocessing-on-gpu>Postprocessing on GPU</a> stage from my previous post is logically closest to our first DeepStream pipeline. This was a fairly slow, early stage in the Python-based optimization journey but limitations in DeepStream around batching and memory transfer make this the best comparison.</p>
<p>This Python-based pipeline runs at around 80 FPS:</p>
<div style="width:100%;height:250px;margin:0 auto">
<canvas id=588></canvas>
</div>
<script src=https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js></script>
<script type=text/javascript>var ctx=document.getElementById('588').getContext('2d'),options=eval("(\n{\n    type: 'horizontalBar',\n    data: {\n        labels: [\n            'Baseline',\n            'Fix Fine-Grained Synchronization',\n            'Postprocessing on GPU',\n            'Batch Processing',\n            'Half Precision Inference',\n            'Decode Video Direct to Device',\n            'Concurrency - 1x2080Ti',\n            'Concurrency - 2x2080Ti'\n        ],\n        datasets: [{\n            label: 'Throughput (FPS)',\n            data: [\n                9,\n                16,\n                80,\n                125,\n                185,\n                235,\n                350,\n                650\n            ],\n            borderColor: [\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(68, 51, 153, 0.5)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n            ],\n            backgroundColor: [\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(68, 51, 153, 0.5)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n            ],\n            borderWidth: 1\n        }]\n    },\n    options: {\n        maintainAspectRatio: false,\n        title: {\n            display: true,\n            text: 'Throughput (FPS)',\n            position: 'top'\n        },\n        scales: {\n            yAxes: [{\n                position: 'right'\n            }],\n            xAxes: [{\n                ticks: {\n                    beginAtZero: true\n                }\n            }]\n        },\n        legend: {\n            display: false\n        }\n    }\n})");new Chart(ctx,options)</script>
<p>After we get a basic DeepStream pipeline up and running we&rsquo;ll empirically understand and then remove the limitations we see.</p>
<h2 id=stage-1-normal-deepstream-mdash-100-torchscript>
Stage 1: Normal DeepStream â€” 100% TorchScript
<a class=anchor href=#stage-1-normal-deepstream-mdash-100-torchscript>#</a>
</h2>
<table>
<thead>
<tr>
<th style=text-align:left>Code</th>
<th style=text-align:left>Nsight Systems Trace</th>
<th style=text-align:left>Gstreamer Pipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left><a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_trt_1.py>ds_trt_1.py</a>, <a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_tsc_1.py>ds_tsc_1.py</a>, <a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_ssd300_1.py>ds_ssd300_1.py</a></td>
<td style=text-align:left><a href=/posts/video-analytics-deepstream-pipeline/logs/ds_1_1gpu_batch16_host.qdrep>ds_1_1gpu_batch16_host.qdrep</a></td>
<td style=text-align:left><a href=/posts/video-analytics-deepstream-pipeline/logs/ds_1_1gpu_batch16_host.pipeline.dot.png>ds_1_1gpu_batch16_host.pipeline.dot.png</a></td>
</tr>
</tbody>
</table>
<p>Our approach to using TorchScript and TensorRT together in a DeepStream pipeline will be to construct a hybrid model with two sequential components â€” a TensorRT frontend passing results to a TorchScript backend which completes the calculation.</p>
<h3 id=hybrid-deepstream-pipeline>
Hybrid DeepStream Pipeline
<a class=anchor href=#hybrid-deepstream-pipeline>#</a>
</h3>
<p>Our hybrid pipeline will eventually use the <code>nvinfer</code> element of DeepStream to serve a TensorRT-compiled form of the SSD300 model directly in the media pipeline. Since TensorRT cannot compile the entire model (due to unsupported <a href=https://onnx.ai/>ONNX</a> ops) we&rsquo;ll run the remaining operations as a TorchScript module (via <a href=https://docs.nvidia.com/metropolis/deepstream/plugin-manual/index.html#page/DeepStream%20Plugins%20Development%20Guide/deepstream_plugin_details.html#wwpID0E0TDB0HA>the <code>parse-bbox-func-name</code> hook</a>).</p>
<p>However, the first pipeline will be the simplest possible while still following the hybrid pattern. The TensorRT model does no processing and simply passes frames to the TorchScript model, which does all preprocessing, inference, and postprocessing. 0% TensorRT, 100% TorchScript.</p>
<p>This pipeline runs at 110 FPS without tracing overhead. However, this TorchScript model has already been converted to <code>fp16</code> precision so a direct comparison to the Python-based pipeline is a bit misleading.</p>
<div style="width:100%;height:275px;margin:0 auto">
<canvas id=426></canvas>
</div>
<script src=https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js></script>
<script type=text/javascript>var ctx=document.getElementById('426').getContext('2d'),options=eval("(\n{\n    type: 'horizontalBar',\n    data: {\n        labels: [\n            'Baseline',\n            'Fix Fine-Grained Synchronization',\n            'Postprocessing on GPU',\n            'Normal DeepStream',\n            'Batch Processing',\n            'Half Precision Inference',\n            'Decode Video Direct to Device',\n            'Concurrency - 1x2080Ti',\n            'Concurrency - 2x2080Ti'\n        ],\n        datasets: [{\n            label: 'Throughput (FPS)',\n            data: [\n                9,\n                16,\n                80,\n                110,\n                125,\n                185,\n                235,\n                350,\n                650\n            ],\n            borderColor: [\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(68, 51, 153, 0.25)',\n                'rgba(118, 185, 0, 0.5)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n            ],\n            backgroundColor: [\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(68, 51, 153, 0.25)',\n                'rgba(118, 185, 0, 0.5)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n            ],\n            borderWidth: 1\n        }]\n    },\n    options: {\n        maintainAspectRatio: false,\n        title: {\n            display: true,\n            text: 'Throughput (FPS)',\n            position: 'top'\n        },\n        scales: {\n            yAxes: [{\n                position: 'right'\n            }],\n            xAxes: [{\n                ticks: {\n                    beginAtZero: true\n                }\n            }]\n        },\n        legend: {\n            display: false\n        }\n    }\n})");new Chart(ctx,options)</script>
<p>Let&rsquo;s drill into the trace with <a href=https://developer.nvidia.com/nsight-systems>Nvidia&rsquo;s Nsight Systems</a> to understand the patterns of execution. I have zoomed in to the processing for two 16-frame batches:</p>
<a href=/posts/video-analytics-deepstream-pipeline/images/ds_100pc_torchscript_normal_ds_two_batch.png>
<figure style="margin:2rem 0">
<img style=max-width:100%;width:auto;height:auto src=/posts/video-analytics-deepstream-pipeline/images/ds_100pc_torchscript_normal_ds_two_batch_hu85cb8d526cf1fda403db89df3e60bf80_432723_896x520_fill_box_top_3.png width=896 height=520>
<figcaption><small></small></figcaption>
</figure>
</a>
<p>Looking at the red NVTX ranges on the <code>GstNvInfer</code> line we can see overlapping ranges where batches of 16 frames are being processed. However, the pattern of processing on the GPU is quite clear from the 16 utilisation spikes â€” it is processing frame-by-frame. We also see constant memory transfers between device and host.</p>
<p>Drilling in to see just two frames of processing, the pattern is even more clear:</p>
<a href=/posts/video-analytics-deepstream-pipeline/images/ds_100pc_torchscript_normal_ds_two_frame.png>
<figure style="margin:2rem 0">
<img style=max-width:100%;width:auto;height:auto src=/posts/video-analytics-deepstream-pipeline/images/ds_100pc_torchscript_normal_ds_two_frame_hudc758a7e69d7157937e6a1d7caab6946_421239_896x540_fill_box_top_3.png width=896 height=540>
<figcaption><small></small></figcaption>
</figure>
</a>
<p>With a little knowledge of how DeepStream works the problem is clear:</p>
<ul>
<li><code>nvinfer</code> sends batches of frames to the configured model engine (our empty TensorRT component) â€” great.</li>
<li><code>nvinfer</code> then sends the model output <em>frame by frame</em> to the postprocessing hook (our TorchScript component).</li>
</ul>
<p>Since we have put our entire model into a TorchScript postprocessing hook we are now processing frame by frame with no batching, and this is causing very low GPU utilisation. (This is why we are comparing against a Python pipeline with no batching).</p>
<p><strong>We are using DeepStream contrary to the design</strong>, but to build a truly hybrid TensorRT and TorchScript pipeline we need batched postprocessing.</p>
<blockquote class="book-hint nvidia">
<p><strong>DeepStream Limitation: Postprocessing Hooks are Frame-by-Frame</strong></p>
<p>The design of <code>nvinfer</code> assumes model output will be postprocessed frame-by-frame. This makes writing postprocessing code a tiny bit easier but is inefficient by default. Preprocessing, inference and postprocessing logic should always assume a batch dimension is present.</p>
</blockquote>
<p>The Nsight Systems view above also shows a pointless sequence of device-to-host then host-to-device transfers. The purple device-to-host memory transfer is due to <code>nvinfer</code> sending tensors to system memory, ready for the postprocessing code to use it. The green host-to-device transfers are me putting this memory back on the GPU where it belongs.</p>
<blockquote class="book-hint nvidia">
<p><strong>DeepStream Limitation: Postprocessing is Assumed to Happen on Host</strong></p>
<p>This is a legacy of early machine learning approaches. Modern deep learning pipelines keep data on the GPU end-to-end, including data augmentation and postprocessing. See Nvidia&rsquo;s <a href=https://developer.nvidia.com/DALI>DALI library</a> for an example of this.</p>
</blockquote>
<p>Okay, time to hack DeepStream and remove these limitations.</p>
<h2 id=stage-2-hacked-deepstream-mdash-100-torchscript>
Stage 2: Hacked DeepStream â€” 100% TorchScript
<a class=anchor href=#stage-2-hacked-deepstream-mdash-100-torchscript>#</a>
</h2>
<table>
<thead>
<tr>
<th style=text-align:left>Code</th>
<th style=text-align:left>Nsight Systems Trace</th>
<th style=text-align:left>Gstreamer Pipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left><a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_trt_2.py>ds_trt_2.py</a>, <a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_tsc_2.py>ds_tsc_2.py</a>, <a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_ssd300_2.py>ds_ssd300_2.py</a></td>
<td style=text-align:left><a href=/posts/video-analytics-deepstream-pipeline/logs/ds_2_1gpu_batch16_device.qdrep>ds_2_1gpu_batch16_device.qdrep</a></td>
<td style=text-align:left><a href=/posts/video-analytics-deepstream-pipeline/logs/ds_2_1gpu_batch16_device.pipeline.dot.png>ds_2_1gpu_batch16_device.pipeline.dot.png</a></td>
</tr>
</tbody>
</table>
<p>Thankfully, Nvidia have provided source for the <code>nvinfer</code> pipeline element. I&rsquo;ve made two changes to better support our approach of doing significant work in the postprocessing hook and fix the above limitations:</p>
<ul>
<li><code>nvinfer</code> model engine output is now sent in a single batch to the postprocessing hook.</li>
<li>Model output tensors are no-longer copied to host, but are left on the device.</li>
</ul>
<blockquote class="book-hint nvidia">
These <code>nvinfer</code> changes are unreleased and are not present in the companion repository (<a href=https://github.com/pbridger/deepstream-video-pipeline>github.com/pbridger/deepstream-video-pipeline</a>) because they are clearly derivative of <code>nvinfer</code> and I&rsquo;m unsure of the licensing. Nvidia people, feel free to get in touch: <a href=mailto:paul@paulbridger.com>paul@paulbridger.com</a>.
</blockquote>
<p>With hacked DeepStream and no model changes at all this pipeline now hits 350 FPS when measured with no tracing overhead. This is up from 110 FPS with regular DeepStream. I think we deserve a chart:</p>
<div style="width:100%;height:300px;margin:0 auto">
<canvas id=105></canvas>
</div>
<script src=https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js></script>
<script type=text/javascript>var ctx=document.getElementById('105').getContext('2d'),options=eval("(\n{\n    type: 'horizontalBar',\n    data: {\n        labels: [\n            'Baseline',\n            'Fix Fine-Grained Synchronization',\n            'Postprocessing on GPU',\n            'Normal DeepStream',\n            'Batch Processing',\n            'Half Precision Inference',\n            'Decode Video Direct to Device',\n            'Hacked DeepStream',\n            'Concurrency - 1x2080Ti',\n            'Concurrency - 2x2080Ti'\n        ],\n        datasets: [{\n            label: 'Throughput (FPS)',\n            data: [\n                9,\n                16,\n                80,\n                110,\n                125,\n                185,\n                235,\n                350,\n                350,\n                650\n            ],\n            borderColor: [\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.25)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.5)',\n                'rgba(68, 51, 153, 0.25)',\n                'rgba(0, 0, 0, 0.1)',\n            ],\n            backgroundColor: [\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.25)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.5)',\n                'rgba(68, 51, 153, 0.25)',\n                'rgba(0, 0, 0, 0.1)',\n            ],\n            borderWidth: 1\n        }]\n    },\n    options: {\n        maintainAspectRatio: false,\n        title: {\n            display: true,\n            text: 'Throughput (FPS)',\n            position: 'top'\n        },\n        scales: {\n            yAxes: [{\n                position: 'right'\n            }],\n            xAxes: [{\n                ticks: {\n                    beginAtZero: true\n                }\n            }]\n        },\n        legend: {\n            display: false\n        }\n    }\n})");new Chart(ctx,options)</script>
<p>The <code>Concurrency 1x2080Ti</code> stage from the Python pipeline is now the closest comparison both in terms of FPS and optimizations applied. Both pipelines have batched inference, video frames decoded and processed on GPU end-to-end, and concurrency at the batch level (note the overlapping NVTX ranges below). One additional level of concurrency in the Python pipeline is multiple overlapping CUDA streams.</p>
<p>The Nsight Systems view shows processing for several 16-frame batches:</p>
<a href=/posts/video-analytics-deepstream-pipeline/images/ds_100pc_torchscript_hacked_ds_two_batch.png>
<figure style="margin:2rem 0">
<img style=max-width:100%;width:auto;height:auto src=/posts/video-analytics-deepstream-pipeline/images/ds_100pc_torchscript_hacked_ds_two_batch_hudc758a7e69d7157937e6a1d7caab6946_445767_896x520_fill_box_top_3.png width=896 height=520>
<figcaption><small></small></figcaption>
</figure>
</a>
<p>We now have good GPU utilization and very few needless memory transfers, so the path forward is to optimize the TorchScript model. Until now the TensorRT component has been entirely pass-through and everything from preprocessing, inference and postprocessing has been in TorchScript.</p>
<p>It&rsquo;s time to start using the TensorRT optimizer, so get ready for some excitement.</p>
<h2 id=stage-3-hacked-deepstream-mdash-80-tensorrt-20-torchscript>
Stage 3: Hacked DeepStream â€” 80% TensorRT, 20% TorchScript
<a class=anchor href=#stage-3-hacked-deepstream-mdash-80-tensorrt-20-torchscript>#</a>
</h2>
<table>
<thead>
<tr>
<th style=text-align:left>Code</th>
<th style=text-align:left>Nsight Systems Trace</th>
<th style=text-align:left>Gstreamer Pipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left><a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_trt_3.py>ds_trt_3.py</a>, <a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_tsc_3.py>ds_tsc_3.py</a>, <a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_ssd300_3.py>ds_ssd300_3.py</a></td>
<td style=text-align:left><a href=/posts/video-analytics-deepstream-pipeline/logs/ds_3_1gpu_batch16_device.qdrep>ds_3_1gpu_batch16_device.qdrep</a></td>
<td style=text-align:left><a href=/posts/video-analytics-deepstream-pipeline/logs/ds_3_1gpu_batch16_device.pipeline.dot.png>ds_3_1gpu_batch16_device.pipeline.dot.png</a></td>
</tr>
</tbody>
</table>
<p>According to Nvidia, TensorRT <a href=https://developer.nvidia.com/tensorrt>&ldquo;dramatically accelerates deep learning inference performance&rdquo;</a> so why not compile 100% of our model with TensorRT?</p>
<p>The Pytorch export to TensorRT consists of a couple of steps, and both provide an opportunity for incomplete support:</p>
<ol>
<li>Export the Pytorch model to the <a href=https://onnx.ai/>ONNX</a> interchange representation via <a href=https://pytorch.org/docs/stable/onnx.html#tracing-vs-scripting>tracing or scripting</a>.</li>
<li>Compile the ONNX representation into a TensorRT engine, the optimized form of the model.</li>
</ol>
<p>If you try to create an optimized TensorRT engine for this entire model (SSD300 including postprocessing), the first problem you will run into is the export to ONNX of the <code>repeat_interleave</code> operation during postprocessing. Pytorch 1.6 does not support this export, I don&rsquo;t know why.</p>
<p>Just like writing C++ in the days before conforming compilers, it&rsquo;s often possible to rewrite model code to work around unsupported operations. See <a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_ssd300_5.py>ds_ssd300_5.py</a> for an example that replaces <code>repeat_interleave</code> and will now export to ONNX. However, now the TensorRT compilation fails with another unsupported operation â€” <code>No importer registered for op: ScatterND</code>.</p>
<p>Dealing with all this is fine if you have a dedicated deployment team â€” simply write custom plugins and CUDA kernels â€” but most teams don&rsquo;t have those resources or time to invest in this.</p>
<p>This is why the hybrid approach works so well â€” we can get the benefits of TensorRT optimization for most of our model and cover the rest with TorchScript.</p>
<p>Speaking of benefits:</p>
<div style="width:100%;height:325px;margin:0 auto">
<canvas id=77></canvas>
</div>
<script src=https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js></script>
<script type=text/javascript>var ctx=document.getElementById('77').getContext('2d'),options=eval("(\n{\n    type: 'horizontalBar',\n    data: {\n        labels: [\n            'Baseline',\n            'Fix Fine-Grained Synchronization',\n            'Postprocessing on GPU',\n            'Normal DeepStream',\n            'Batch Processing',\n            'Half Precision Inference',\n            'Decode Video Direct to Device',\n            'Hacked DeepStream',\n            'Concurrency - 1x2080Ti',\n            'Concurrency - 2x2080Ti',\n            'TensorRT'\n        ],\n        datasets: [{\n            label: 'Throughput (FPS)',\n            data: [\n                9,\n                16,\n                80,\n                110,\n                125,\n                185,\n                235,\n                350,\n                350,\n                650,\n                920\n            ],\n            borderColor: [\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.25)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.25)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.5)',\n            ],\n            backgroundColor: [\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.25)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.25)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.5)',\n            ],\n            borderWidth: 1\n        }]\n    },\n    options: {\n        maintainAspectRatio: false,\n        title: {\n            display: true,\n            text: 'Throughput (FPS)',\n            position: 'top'\n        },\n        scales: {\n            yAxes: [{\n                position: 'right'\n            }],\n            xAxes: [{\n                ticks: {\n                    beginAtZero: true\n                }\n            }]\n        },\n        legend: {\n            display: false\n        }\n    }\n})");new Chart(ctx,options)</script>
<p>920 FPS up from 350 FPS is a huge jump, and we are still only using a single 2080Ti GPU. Let&rsquo;s check Nsight Systems to understand how this can be possible:</p>
<a href=/posts/video-analytics-deepstream-pipeline/images/ds_80pc_tensorrt_hacked_ds_two_batch.png>
<figure style="margin:2rem 0">
<img style=max-width:100%;width:auto;height:auto src=/posts/video-analytics-deepstream-pipeline/images/ds_80pc_tensorrt_hacked_ds_two_batch_hudc758a7e69d7157937e6a1d7caab6946_489589_896x520_fill_box_top_3.png width=896 height=520>
<figcaption><small></small></figcaption>
</figure>
</a>
<p>Two important things to note:</p>
<ul>
<li>TensorRT inference for batch N is now interleaved/concurrent with TorchScript postprocessing for batch N-1, helping to fill in utilization gaps.</li>
<li>The TensorRT preprocessing and inference are massively faster than the TorchScript version. Around 43ms of TorchScript preprocessing and inference have turned into around 16ms of equivalent TensorRT processing.</li>
</ul>
<p>This Nsight Systems trace output now looks a little like what we were aiming for:</p>
<p><img src=/posts/video-analytics-deepstream-pipeline/images/deepstream_hybrid.svg alt="DeepStream Hybrid Architecture"></p>
<p>Given the awesome improvement TensorRT gave us, did we really need to hack DeepStream?</p>
<h2 id=stage-4-normal-deepstream-mdash-80-tensorrt-20-torchscript>
Stage 4: Normal DeepStream â€” 80% TensorRT, 20% TorchScript
<a class=anchor href=#stage-4-normal-deepstream-mdash-80-tensorrt-20-torchscript>#</a>
</h2>
<table>
<thead>
<tr>
<th style=text-align:left>Code</th>
<th style=text-align:left>Nsight Systems Trace</th>
<th style=text-align:left>Gstreamer Pipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left><a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_trt_4.py>ds_trt_4.py</a>, <a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_tsc_4.py>ds_tsc_4.py</a>, <a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_ssd300_4.py>ds_ssd300_4.py</a></td>
<td style=text-align:left><a href=/posts/video-analytics-deepstream-pipeline/logs/ds_4_1gpu_batch16_host.qdrep>ds_4_1gpu_batch16_host.qdrep</a></td>
<td style=text-align:left><a href=/posts/video-analytics-deepstream-pipeline/logs/ds_4_1gpu_batch16_host.pipeline.dot.png>ds_4_1gpu_batch16_host.pipeline.dot.png</a></td>
</tr>
</tbody>
</table>
<p>In short, yes, we did need to hack DeepStream to get the best throughput. Unless you like the sound of 360 FPS when you could be hitting 920 FPS. This is a step backwards so I&rsquo;m not adding it to our chart.</p>
<p>Here is the trace when we run the TensorRT-optimized model with the TorchScript final processing:</p>
<a href=/posts/video-analytics-deepstream-pipeline/images/ds_80pc_tensorrt_normal_ds_two_batch.png>
<figure style="margin:2rem 0">
<img style=max-width:100%;width:auto;height:auto src=/posts/video-analytics-deepstream-pipeline/images/ds_80pc_tensorrt_normal_ds_two_batch_hudc758a7e69d7157937e6a1d7caab6946_427650_896x550_fill_box_top_3.png width=896 height=550>
<figcaption><small></small></figcaption>
</figure>
</a>
<p>The problems are pretty clear, as annotated in the trace.</p>
<blockquote class="book-hint nvidia">
<p><strong>DeepStream is Awesome</strong></p>
<p>But Hacked DeepStream is even better. :D</p>
</blockquote>
<h2 id=stage-5-horizontal-scalability>
Stage 5: Horizontal Scalability
<a class=anchor href=#stage-5-horizontal-scalability>#</a>
</h2>
<table>
<thead>
<tr>
<th style=text-align:left>Code</th>
<th style=text-align:left>Nsight Systems Trace</th>
<th style=text-align:left>Gstreamer Pipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left><a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_trt_3.py>ds_trt_3.py</a>, <a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_tsc_3.py>ds_tsc_3.py</a>, <a href=https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_ssd300_3.py>ds_ssd300_3.py</a></td>
<td style=text-align:left><a href=/posts/video-analytics-deepstream-pipeline/logs/ds_3_2gpu_batch16_device.qdrep>ds_3_2gpu_batch16_device.qdrep</a></td>
<td style=text-align:left><a href=/posts/video-analytics-deepstream-pipeline/logs/ds_3_2gpu_batch16_device.pipeline.dot.png>ds_3_2gpu_batch16_device.pipeline.dot.png</a></td>
</tr>
</tbody>
</table>
<p>Doubling the hardware available to our Python-based pipeline boosted throughput from 350 FPS to 650 FPS, around an 86% increase. This was a single Python process driving two very powerful GPUs so it&rsquo;s a great result. Given the measured GIL contention at around 45% scaling further would have become less efficient, perhaps requiring a multi-process approach.</p>
<p>Our DeepStream pipelines have been launched from Python, but with no callbacks beyond an empty once-per-second message loop so there is no chance of GIL contention. Measured without tracing overhead these DeepStream pipelines show perfect 100% scalability (at least from 1 to 2 devices), topping out at 1840 FPS. It&rsquo;s like Christmas morning.</p>
<div style="width:100%;height:350px;margin:0 auto">
<canvas id=89></canvas>
</div>
<script src=https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js></script>
<script type=text/javascript>var ctx=document.getElementById('89').getContext('2d'),options=eval("(\n{\n    type: 'horizontalBar',\n    data: {\n        labels: [\n            'Baseline',\n            'Fix Fine-Grained Synchronization',\n            'Postprocessing on GPU',\n            'Normal DeepStream',\n            'Batch Processing',\n            'Half Precision Inference',\n            'Decode Video Direct to Device',\n            'Hacked DeepStream',\n            'Concurrency - 1x2080Ti',\n            'Concurrency - 2x2080Ti',\n            'TensorRT - 1x2080Ti',\n            'TensorRT - 2x2080Ti',\n        ],\n        datasets: [{\n            label: 'Throughput (FPS)',\n            data: [\n                9,\n                16,\n                80,\n                110,\n                125,\n                185,\n                235,\n                350,\n                350,\n                650,\n                920,\n                1840\n            ],\n            borderColor: [\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.25)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.25)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.25)',\n                'rgba(118, 185, 0, 0.5)',\n            ],\n            backgroundColor: [\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.25)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.25)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(0, 0, 0, 0.1)',\n                'rgba(118, 185, 0, 0.25)',\n                'rgba(118, 185, 0, 0.5)',\n            ],\n            borderWidth: 1\n        }]\n    },\n    options: {\n        maintainAspectRatio: false,\n        title: {\n            display: true,\n            text: 'Throughput (FPS)',\n            position: 'top'\n        },\n        scales: {\n            yAxes: [{\n                position: 'right'\n            }],\n            xAxes: [{\n                ticks: {\n                    beginAtZero: true\n                }\n            }]\n        },\n        legend: {\n            display: false\n        }\n    }\n})");new Chart(ctx,options)</script>
<p>Incidentally, whereas most of the previous stages suffer from a roughly 15% hit to throughput with Nsight Systems tracing enabled this pipeline takes a 40% drop. You&rsquo;ll see this difference if you download and analyze the linked trace files.</p>
<h2 id=conclusion>
Conclusion
<a class=anchor href=#conclusion>#</a>
</h2>
<p>We have a pipeline capable of doing 1840 FPS of useful object detection throughput, and this is phenomenal. This should convincingly demonstrate the effectiveness of these technologies working together.</p>
<p>Despite the huge gains delivered by TensorRT optimization and the efficient scalability of DeepStream, <strong>TorchScript is the unsung hero of this story</strong>. The ability to easily export any Pytorch model without worrying about missing layers or operations is huge. Without TorchScript and <code>libtorch</code> I would still be writing TensorRT plugins.</p>
<p>In future articles I&rsquo;ll delve deeper into the TorchScript export process and explain some of the portability and performance pitfalls.</p>
<h3 id=caveats-limitations-and-excuses>
Caveats, Limitations and Excuses
<a class=anchor href=#caveats-limitations-and-excuses>#</a>
</h3>
<p>The Gstreamer/DeepStream pipelines used above do not reflect 100% realistic usage. If you review the pipeline diagrams (e.g. <a href=/posts/video-analytics-deepstream-pipeline/logs/ds_3_2gpu_batch16_device.pipeline.dot.png>ds_3_2gpu_batch16_device.pipeline.dot.png</a>) you&rsquo;ll see a single file is being read and piped into the <code>nvstreammux</code> component many times. This is how you would handle multiple concurrent media streams into a single inference engine, but the real reason I&rsquo;ve done this is to work around a <a href=https://forums.developer.nvidia.com/t/ds-5-0-1-nvstreammux-batch-size-bug/157039>limitation of <code>nvstreammux</code></a> to do with batching. Read the linked issue for the details, but it is fair to say that <code>nvstreammux</code> is not intended for assembling efficiently-sized batches when processing a small number of input streams.</p>
<p>Also as noted above, my &ldquo;Hacked DeepStream&rdquo; code is not yet publically available. I&rsquo;ll work to tidy this up and if I&rsquo;m sure of the licensing situation I&rsquo;ll make this available.</p>
<p>Finally the code in the <a href=https://github.com/pbridger/deepstream-video-pipeline>associated repository</a> is not polished tutorial code, it is hacky research code so caveat emptor.</p>
</article>
<article class="markdown social">
<hr>
<section class=social-share>
<ul class=share-icons>
<li>
<a href target=_blank class="share-btn newsletter"><svg class="widget-social__link-icon icon icon-mail" fill="#fff" width="24" height="24" viewBox="0 0 166.781 166.781"><g><g><g><path d="M163.451 70.046l-32.35-20.847c-.253-.161-.532-.222-.804-.312v-7.19c0-1.92-1.554-3.475-3.475-3.475H113.92L86.97 21.378c-1.126-.706-2.558-.706-3.685.0l-26.95 16.844H39.958c-1.92.0-3.475 1.554-3.475 3.475v7.188c-.272.09-.552.152-.804.314L3.329 70.046c-.991.641-1.592 1.741-1.592 2.921v90.339c0 1.92 1.554 3.475 3.475 3.475h156.356c1.92.0 3.475-1.554 3.475-3.475V72.968C165.043 71.787 164.442 70.688 163.451 70.046zM85.128 28.423l15.681 9.799H69.447l15.681-9.799zM43.433 45.171h79.915v78.178c0 .01.006.018.006.029l-11.754 7.137-28.284-15.427c-1.055-.57-2.338-.567-3.386.034l-25.81 14.749-10.692-6.492c0-.01.006-.018.006-.028L43.433 45.171zM8.687 74.861l27.796-17.91v62.212L8.687 102.285V74.861zm0 35.551 38.537 23.397L8.687 155.831V110.412zm7.002 49.421 66.005-37.715 69.145 37.715H15.689zm142.405-3.959L118.65 134.36l39.444-23.949v45.463zm0-53.589-27.797 16.877V56.951l27.797 17.911v27.423z"/><path d="M57.331 79.917h41.695c1.92.0 3.475-1.554 3.475-3.475V55.595c0-1.92-1.554-3.475-3.475-3.475H57.331c-1.92.0-3.475 1.554-3.475 3.475v20.847C53.856 78.363 55.411 79.917 57.331 79.917zm3.474-20.848h34.746v13.898H60.805V59.069z"/><rect x="53.856" y="86.866" width="55.593" height="6.949"/><rect x="53.856" y="100.765" width="55.593" height="6.949"/><path d="M147.67 41.697c.889.0 1.778-.339 2.457-1.018l12.283-12.283c1.357-1.357 1.357-3.556.0-4.913-1.357-1.358-3.556-1.357-4.913.0l-12.283 12.283c-1.357 1.357-1.357 3.556.0 4.913C145.892 41.358 146.781 41.697 147.67 41.697z"/><path d="M16.654 40.679c.679.679 1.568 1.018 2.457 1.018s1.778-.339 2.457-1.018c1.357-1.357 1.357-3.556.0-4.913L9.284 23.483c-1.357-1.357-3.556-1.357-4.913.0s-1.357 3.556.0 4.913L16.654 40.679z"/><path d="M118.584 24.076c.421.17.859.247 1.289.247 1.378.0 2.684-.825 3.227-2.185l6.949-17.373c.713-1.781-.156-3.804-1.937-4.516-1.764-.709-3.804.149-4.516 1.937l-6.949 17.373C115.934 21.341 116.802 23.364 118.584 24.076z"/><path d="M47.155 22.139c.543 1.361 1.849 2.185 3.227 2.185.431.0.869-.078 1.289-.248 1.781-.713 2.65-2.735 1.937-4.516L46.659 2.187C45.946.399 43.911-.46 42.143.25c-1.781.713-2.65 2.735-1.937 4.516l6.949 17.373z"/></g></g></g></svg>
<p>Newsletter</p>
</a>
</li>
<li class=share-label>
<script async data-uid=d104ecfe6c src=https://paulbridger.ck.page/d104ecfe6c/index.js></script>
</li>
</ul>
<ul class=share-icons>
<li>
<a href="https://twitter.com/intent/follow?ref_src=twsrc%5Etfw&region=follow_link&screen_name=paul_bridger&tw_p=followbutton" target=_blank rel=noopener aria-label="Follow on Twitter" class="share-btn twitter"><svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path fill="#fff" d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5.0-78.8 35.3-78.8 78.8.0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3C20 26 16.1 39.6 16.1 54c0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1.0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4.0-12.6-.4-18.8-1.1C34.9 299 76.3 312 120.8 312c144.9.0 224.1-120 224.1-224.1.0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
<p>Twitter</p>
</a>
</li>
<li class=share-label>
<a href="https://twitter.com/intent/follow?ref_src=twsrc%5Etfw&region=follow_link&screen_name=paul_bridger&tw_p=followbutton" target=_blank rel=noopener aria-label="Follow on Twitter">
Follow on Twitter
</a>
</li>
</ul>
<ul class=share-icons>
<li>
<a href=https://www.linkedin.com/in/paulbridger/ target=_blank rel=noopener aria-label="Connect on LinkedIn" class="share-btn linkedin"><svg class="widget-social__link-icon icon icon-linkedin" width="24" height="24" viewBox="0 0 352 352"><path fill="#fff" d="M0 40v272c0 21.9 18.1 40 40 40h272c21.9.0 40-18.1 40-40V40c0-21.9-18.1-40-40-40H40C18.1.0.0 18.1.0 40zm312-8c4.6.0 8 3.4 8 8v272c0 4.6-3.4 8-8 8H40c-4.6.0-8-3.4-8-8V40c0-4.6 3.4-8 8-8H312zM59.5 87c0 15.2 12.3 27.5 27.5 27.5s27.5-12.3 27.5-27.5S102.2 59.5 87 59.5 59.5 71.8 59.5 87zM187 157h-1v-21h-45v152h47v-75c0-19.8 3.9-39 28.5-39 24.2.0 24.5 22.4 24.5 40v74h47v-83.5c0-40.9-8.7-72-56.5-72-23 0-38.2 12.6-44.5 24.5zM64 288h47.5V136H64V288z"/></svg>
<p>LinkedIn</p>
</a>
</li>
<li class=share-label>
<a href=https://www.linkedin.com/in/paulbridger/ target=_blank rel=noopener aria-label="Connect on LinkedIn">
Connect on LinkedIn
</a>
</li>
</ul>
<ul class=share-icons>
<li>
<a href=mailto:paul@paulbridger.com target=_blank class="share-btn email"><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path fill="#fff" d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg>
<p>Email</p>
</a>
</li>
<li class=share-label>
<a href=mailto:paul@paulbridger.com target=_blank>
Contact by Email
</a>
</li>
</ul>
<ul class=share-icons>
<li>
<a href="https://news.ycombinator.com/item?id=24817173" class="share-btn yc"><svg class="widget-social__link-icon icon icon-yc" width="24" height="24" viewBox="3 3 24 24"><path fill="#fff" d="M14 17 8.8 7.3h2.4l3 6.1c0 .1.1.2.2.3.1.1.1.2.2.4l.1.1s0 .1.0.1c.1.2.1.3.2.5.1.1.1.3.2.4.1-.3.3-.5.4-.9.1-.3.3-.6.5-.9l3-6.1h2.2L16 17.1v6.2h-2V17z"/></svg>
<p>Hacker News</p>
</a>
</li>
<li class=share-label>
<a href="https://news.ycombinator.com/item?id=24817173" target=_blank>
Discuss on HN
</a>
</li>
</ul>
</section>
</article>
<footer class=book-footer>
<div class="flex flex-wrap justify-between">
</div>
<script>(function(){function a(c){const a=window.getSelection(),b=document.createRange();b.selectNodeContents(c),a.removeAllRanges(),a.addRange(b)}document.querySelectorAll("pre code").forEach(b=>{b.addEventListener("click",function(c){a(b.parentElement),navigator.clipboard&&navigator.clipboard.writeText(b.parentElement.textContent)})})})()</script>
<hr>
<b>&copy; Paul Bridger 2020</b>
</footer>
<div class=book-comments>
</div>
<label for=menu-control class="hidden book-menu-overlay"></label>
</div>
<aside class=book-toc>
<div class=book-toc-content>
<div class=book-toc-div>
<nav id=TableOfContents>
<ul>
<li><a href=#intro>Intro</a>
<ul>
<li><a href=#torchscript-vs-tensorrt>ðŸ”¥TorchScript vs TensorRTðŸ”¥</a></li>
</ul>
</li>
<li><a href=#stage-0-python-baseline>Stage 0: Python Baseline</a></li>
<li><a href=#stage-1-normal-deepstream-mdash-100-torchscript>Stage 1: Normal DeepStream â€” 100% TorchScript</a>
<ul>
<li><a href=#hybrid-deepstream-pipeline>Hybrid DeepStream Pipeline</a></li>
</ul>
</li>
<li><a href=#stage-2-hacked-deepstream-mdash-100-torchscript>Stage 2: Hacked DeepStream â€” 100% TorchScript</a></li>
<li><a href=#stage-3-hacked-deepstream-mdash-80-tensorrt-20-torchscript>Stage 3: Hacked DeepStream â€” 80% TensorRT, 20% TorchScript</a></li>
<li><a href=#stage-4-normal-deepstream-mdash-80-tensorrt-20-torchscript>Stage 4: Normal DeepStream â€” 80% TensorRT, 20% TorchScript</a></li>
<li><a href=#stage-5-horizontal-scalability>Stage 5: Horizontal Scalability</a></li>
<li><a href=#conclusion>Conclusion</a>
<ul>
<li><a href=#caveats-limitations-and-excuses>Caveats, Limitations and Excuses</a></li>
</ul>
</li>
</ul>
</nav>
<nav>
Have a question or comment?<br>
<a href="https://news.ycombinator.com/item?id=24817173">Join the discussion on Hacker News <img src=/images/y.svg></a>
</nav>
</div>
</div>
</aside>
</main>
</body>
</html>