<!doctype html><html lang=en dir=ltr>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="Taking machine learning models into production for video analytics doesn&rsquo;t have to be hard. A pipeline with reasonable efficiency can be created very quickly just by plugging together the right libraries. In this post we&rsquo;ll create a video pipeline with a focus on flexibility and simplicity using two main libraries: Gstreamer and Pytorch.">
<meta name=theme-color content="#FFFFFF"><meta property="og:title" content="A Simple and Flexible Pytorch Video Pipeline">
<meta property="og:description" content="Taking machine learning models into production for video analytics doesn&rsquo;t have to be hard. A pipeline with reasonable efficiency can be created very quickly just by plugging together the right libraries. In this post we&rsquo;ll create a video pipeline with a focus on flexibility and simplicity using two main libraries: Gstreamer and Pytorch.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://paulbridger.com/posts/video-analytics-pytorch-pipeline/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-09-23T18:21:00+02:00">
<meta property="article:modified_time" content="2020-09-23T18:21:00+02:00">
<title>A Simple and Flexible Pytorch Video Pipeline | paulbridger.com</title>
<link rel=manifest href=/manifest.json>
<link rel=icon href=/favicon.png type=image/x-icon>
<link rel=stylesheet href=/book.min.b5056d4fbae4a365a3e7a6f0af8a29daf564e71fa285cacf5f41e0c78741630d.css integrity="sha256-tQVtT7rko2Wj56bwr4op2vVk5x+ihcrPX0Hgx4dBYw0=" crossorigin=anonymous>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-3441595-4','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
</head>
<body dir=ltr>
<input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control>
<main class="container flex">
<aside class=book-menu>
<div class=book-menu-content>
<nav>
<h2 class=book-brand>
<a href=/><span>paulbridger.com</span>
</a>
</h2>
<ul>
<li>
<a href=https://paulbridger.com/posts/pytorch-memory-tuning/>PyTorch Memory Tuning</a>
</li>
<li>
<a href=https://paulbridger.com/posts/pytorch-tuning-tips/>PyTorch Performance Features and How They Interact</a>
</li>
<li>
<a href=https://paulbridger.com/posts/nsight-systems-systematic-optimization/>Solving Machine Learning Performance Anti-Patterns: a Systematic Approach</a>
</li>
<li>
<a href=https://paulbridger.com/posts/video-analytics-pytorch-pipeline/ class=active>A Simple and Flexible Pytorch Video Pipeline</a>
</li>
<li>
<a href=https://paulbridger.com/posts/video-analytics-pipeline-tuning/>Object Detection from 9 FPS to 650 FPS in 6 Steps</a>
</li>
<li>
<a href=https://paulbridger.com/posts/video-analytics-deepstream-pipeline/>Object Detection at 1840 FPS with TorchScript, TensorRT and DeepStream</a>
</li>
<li>
<a href=https://paulbridger.com/posts/tensorrt-object-detection-quantized/>Object Detection at 2530 FPS with TensorRT and 8-Bit Quantization</a>
</li>
<li>
<a href=https://paulbridger.com/posts/mastering-torchscript/>Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification</a>
</li>
<li>
<a href=https://paulbridger.com/posts/consulting/>Consulting / Hire Me</a>
</li>
<li>
<a href=https://paulbridger.com/posts/about/>About Paul Bridger</a>
</li>
</ul>
</nav>
<script>(function(){var a=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>
</div>
</aside>
<div class=book-page>
<header class=book-header>
<div class="flex align-center justify-between">
<label for=menu-control>
<img src=/svg/menu.svg class=book-icon alt=Menu>
</label>
<strong>A Simple and Flexible Pytorch Video Pipeline</strong>
<label for=toc-control>
<img src=/svg/toc.svg class=book-icon alt="Table of Contents">
</label>
</div>
<aside class="hidden clearfix">
<div class=book-toc-div>
<nav id=TableOfContents>
<ul>
<li><a href=#intro>Intro</a></li>
<li><a href=#step-0-baseline-cli-gstreamer-pipeline>Step 0: Baseline CLI Gstreamer pipeline</a></li>
<li><a href=#step-1-get-frames-into-python>Step 1: Get frames into Python</a></li>
<li><a href=#step-2-get-frames-into-pytorch>Step 2: Get frames into Pytorch</a></li>
<li><a href=#conclusion>Conclusion</a></li>
</ul>
</nav>
<nav>
</nav>
</div>
</aside>
</header>
<article class=markdown>
<h1>
<a href=/posts/video-analytics-pytorch-pipeline/>A Simple and Flexible Pytorch Video Pipeline</a>
</h1>
<h5>September 23, 2020</h5>
<div>
<a href=/categories/visual-analytics/>Visual Analytics</a>
</div>
<div>
<a href=/tags/ssd300/>SSD300</a>,
<a href=/tags/pytorch/>Pytorch</a>,
<a href=/tags/object-detection/>Object Detection</a>,
<a href=/tags/gstreamer/>Gstreamer</a>
</div>
<h2 id=intro>
Intro
<a class=anchor href=#intro>#</a>
</h2>
<p>Taking machine learning models into production for video analytics doesn&rsquo;t have to be hard. A pipeline with reasonable efficiency can be created very quickly just by plugging together the right libraries. In this post we&rsquo;ll create a video pipeline with a focus on flexibility and simplicity using two main libraries: <a href=https://gstreamer.freedesktop.org/>Gstreamer</a> and <a href=https://pytorch.org>Pytorch</a>.</p>
<p>Performance is out of scope for this first step, but we&rsquo;ll do a deep dive in a later article. Worthwhile throughput improvements are possible with a little effort. We&rsquo;ll also ignore black-box serving toolkits (Nvidia Triton/TensorRT, Kubeflow, TorchServe etc.) so we can understand what&rsquo;s happening end to end.</p>
<p>The main library we&rsquo;ll be using is <a href=https://gstreamer.freedesktop.org/>Gstreamer</a>, a very flexible and efficient media-processing pipeline that comes with a huge ecosystem of components. In principle these components can be seamlessly swapped out to support different codecs, transformations and outputs but in practice constructing a Gstreamer pipeline can be a tricky process with a lot of iteration.</p>
<p>I&rsquo;ve created a repo with some example code here: <a href=https://github.com/pbridger/pytorch-video-pipeline>https://github.com/pbridger/pytorch-video-pipeline</a>
Besides the code, the repo contains a Dockerfile and top-level Makefile to make running the scripts easy.</p>
<h2 id=step-0-baseline-cli-gstreamer-pipeline>
Step 0: Baseline CLI Gstreamer pipeline
<a class=anchor href=#step-0-baseline-cli-gstreamer-pipeline>#</a>
</h2>
<p>In order to show the basic Gstreamer pipeline components and to validate the container environment, we can run something like this from the CLI:</p>
<pre tabindex=0><code>$ gst-launch-1.0 filesrc location=media/in.mp4 ! decodebin ! progressreport update-freq=1 ! fakesink sync=true
</code></pre><p>Running this will show the video file being read (by the <code>filesrc</code> element), decoded (<code>decodebin</code> element) and sent to the Gstreamer equivalent of /dev/null (<code>fakesink</code> element).</p>
<p>If you don&rsquo;t have Gstreamer installed, the easiest way to do this is to use the makefile from the repo. Grab the repo from github, then use this make target:</p>
<pre tabindex=0><code>$ make cli.pipeline.png
</code></pre><p>Alternatively, start the Docker container using the makefile and run the above gst-launch-1.0 command from within:</p>
<pre tabindex=0><code>$ make run-container
...
/app# gst-launch-1.0 filesrc location=media/in.mp4 ! decodebin ! progressreport update-freq=1 ! fakesink sync=true
</code></pre><p>Output should look something like this:</p>
<details>
<summary>Output: CLI Gstreamer</summary>
<div>
<figure style="margin:2rem 0">
<img src=/posts/video-analytics-pytorch-pipeline/images/cli.output.svg>
</figure>
</div>
</details>
<p>Gstreamer is able to generate a representation showing the transformations in the pipeline, see below:</p>
<details>
<summary>Pipeline: CLI Gstreamer</summary>
<div>
<a href=/posts/video-analytics-pytorch-pipeline/images/cli.pipeline.png>
<figure style="margin:2rem 0">
<img style=max-width:100%;width:auto;height:auto src=/posts/video-analytics-pytorch-pipeline/images/cli.pipeline_hu1961d2a9818f6bc6d76dbf2f043727e4_198140_800x0_resize_box_3.png width=800 height=54>
<figcaption><small></small></figcaption>
</figure>
</a>
</div>
</details>
<h2 id=step-1-get-frames-into-python>
Step 1: Get frames into Python
<a class=anchor href=#step-1-get-frames-into-python>#</a>
</h2>
<p>Since we want to feed these frames into a Pytorch model running in the Python runtime we&rsquo;ll construct a similar pipeline from a script:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>os</span><span class=o>,</span> <span class=nn>sys</span>
<span class=kn>import</span> <span class=nn>gi</span>
<span class=n>gi</span><span class=o>.</span><span class=n>require_version</span><span class=p>(</span><span class=s1>&#39;Gst&#39;</span><span class=p>,</span> <span class=s1>&#39;1.0&#39;</span><span class=p>)</span>
<span class=kn>from</span> <span class=nn>gi.repository</span> <span class=kn>import</span> <span class=n>Gst</span>

<span class=n>frame_format</span> <span class=o>=</span> <span class=s1>&#39;RGBA&#39;</span>

<span class=n>Gst</span><span class=o>.</span><span class=n>init</span><span class=p>()</span>
<span class=n>pipeline</span> <span class=o>=</span> <span class=n>Gst</span><span class=o>.</span><span class=n>parse_launch</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;&#39;&#39;
</span><span class=s1>    filesrc location=media/in.mp4 num-buffers=200 !
</span><span class=s1>    decodebin !
</span><span class=s1>    fakesink name=s
</span><span class=s1>&#39;&#39;&#39;</span><span class=p>)</span>

<span class=k>def</span> <span class=nf>on_frame_probe</span><span class=p>(</span><span class=n>pad</span><span class=p>,</span> <span class=n>info</span><span class=p>):</span>
    <span class=n>buf</span> <span class=o>=</span> <span class=n>info</span><span class=o>.</span><span class=n>get_buffer</span><span class=p>()</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;[</span><span class=si>{</span><span class=n>buf</span><span class=o>.</span><span class=n>pts</span> <span class=o>/</span> <span class=n>Gst</span><span class=o>.</span><span class=n>SECOND</span><span class=si>:</span><span class=s1>6.2f</span><span class=si>}</span><span class=s1>]&#39;</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>Gst</span><span class=o>.</span><span class=n>PadProbeReturn</span><span class=o>.</span><span class=n>OK</span>

<span class=n>pipeline</span><span class=o>.</span><span class=n>get_by_name</span><span class=p>(</span><span class=s1>&#39;s&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>get_static_pad</span><span class=p>(</span><span class=s1>&#39;sink&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>add_probe</span><span class=p>(</span>
    <span class=n>Gst</span><span class=o>.</span><span class=n>PadProbeType</span><span class=o>.</span><span class=n>BUFFER</span><span class=p>,</span>
    <span class=n>on_frame_probe</span>
<span class=p>)</span>

<span class=n>pipeline</span><span class=o>.</span><span class=n>set_state</span><span class=p>(</span><span class=n>Gst</span><span class=o>.</span><span class=n>State</span><span class=o>.</span><span class=n>PLAYING</span><span class=p>)</span>

<span class=k>try</span><span class=p>:</span>
    <span class=k>while</span> <span class=kc>True</span><span class=p>:</span>
        <span class=n>msg</span> <span class=o>=</span> <span class=n>pipeline</span><span class=o>.</span><span class=n>get_bus</span><span class=p>()</span><span class=o>.</span><span class=n>timed_pop_filtered</span><span class=p>(</span>
            <span class=n>Gst</span><span class=o>.</span><span class=n>SECOND</span><span class=p>,</span>
            <span class=n>Gst</span><span class=o>.</span><span class=n>MessageType</span><span class=o>.</span><span class=n>EOS</span> <span class=o>|</span> <span class=n>Gst</span><span class=o>.</span><span class=n>MessageType</span><span class=o>.</span><span class=n>ERROR</span>
        <span class=p>)</span>
        <span class=k>if</span> <span class=n>msg</span><span class=p>:</span>
            <span class=n>text</span> <span class=o>=</span> <span class=n>msg</span><span class=o>.</span><span class=n>get_structure</span><span class=p>()</span><span class=o>.</span><span class=n>to_string</span><span class=p>()</span> <span class=k>if</span> <span class=n>msg</span><span class=o>.</span><span class=n>get_structure</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;&#39;</span>
            <span class=n>msg_type</span> <span class=o>=</span> <span class=n>Gst</span><span class=o>.</span><span class=n>message_type_get_name</span><span class=p>(</span><span class=n>msg</span><span class=o>.</span><span class=n>type</span><span class=p>)</span>
            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>msg</span><span class=o>.</span><span class=n>src</span><span class=o>.</span><span class=n>name</span><span class=si>}</span><span class=s1>: [</span><span class=si>{</span><span class=n>msg_type</span><span class=si>}</span><span class=s1>] </span><span class=si>{</span><span class=n>text</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
            <span class=k>break</span>
<span class=k>finally</span><span class=p>:</span>
    <span class=nb>open</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;logs/</span><span class=si>{</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>splitext</span><span class=p>(</span><span class=n>sys</span><span class=o>.</span><span class=n>argv</span><span class=p>[</span><span class=mi>0</span><span class=p>])[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=s1>.pipeline.dot&#39;</span><span class=p>,</span> <span class=s1>&#39;w&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>write</span><span class=p>(</span>
        <span class=n>Gst</span><span class=o>.</span><span class=n>debug_bin_to_dot_data</span><span class=p>(</span><span class=n>pipeline</span><span class=p>,</span> <span class=n>Gst</span><span class=o>.</span><span class=n>DebugGraphDetails</span><span class=o>.</span><span class=n>ALL</span><span class=p>)</span>
    <span class=p>)</span>
    <span class=n>pipeline</span><span class=o>.</span><span class=n>set_state</span><span class=p>(</span><span class=n>Gst</span><span class=o>.</span><span class=n>State</span><span class=o>.</span><span class=n>NULL</span><span class=p>)</span>
</code></pre></div>
<p>The above code runs the same filesrc-decode pipeline, monitoring the pipeline for errors and end of stream (EOS) messages, and installs a probe callback (<code>on_frame_probe</code>) which will be called for every frame processed. This is about as simple as I could make it. With this code we have video frames/buffers available within the Python callback as Gstreamer buffers.</p>
<p>To run this using the makefile:</p>
<pre tabindex=0><code>$ make frames_into_python.pipeline.png
</code></pre><p>As you can see from the <code>Gst.parse_launch</code> call the constructed pipeline is even simpler than the CLI version since we don&rsquo;t bother with the <code>progressreport</code> element. Also, because we removed the <code>sync=true</code> parameter from the fakesink element the pipeline runs as fast as the slowest pipeline element instead of synchronizing with the clock:</p>
<details>
<summary>Output: frames_into_python.py</summary>
<div>
<figure style="margin:2rem 0">
<img src=/posts/video-analytics-pytorch-pipeline/images/frames_into_python.output.svg>
</figure>
</div>
</details>
<details>
<summary>Pipeline: frames_into_python.py</summary>
<div>
<a href=/posts/video-analytics-pytorch-pipeline/images/frames_into_python.pipeline.png>
<figure style="margin:2rem 0">
<img style=max-width:100%;width:auto;height:auto src=/posts/video-analytics-pytorch-pipeline/images/frames_into_python.pipeline_hu8862f21746f9d6519809566160033120_170616_800x0_resize_box_3.png width=800 height=60>
<figcaption><small></small></figcaption>
</figure>
</a>
</div>
</details>
<h2 id=step-2-get-frames-into-pytorch>
Step 2: Get frames into Pytorch
<a class=anchor href=#step-2-get-frames-into-pytorch>#</a>
</h2>
<p>Now we&rsquo;ll include logic to add two things to the <code>on_frame_probe</code> callback:</p>
<ol>
<li>Reinterpret and copy the decoded Gstreamer buffer into Pytorch tensor.</li>
<li>Run some basic object detection on the image tensor using <a href=https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/>Nvidia&rsquo;s SSD300</a>.</li>
</ol>
<p>In the interests of keeping the code short and simple this sample has some deliberate limitations:</p>
<ul>
<li>Incomplete image preprocessing.</li>
<li>No inference post-processing, so we don&rsquo;t even get bounding boxes to print.</li>
<li>No emphasis whatsoever on performance except for running on CUDA/GPU if available.</li>
</ul>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>os</span><span class=o>,</span> <span class=nn>sys</span>
<span class=kn>import</span> <span class=nn>gi</span>
<span class=n>gi</span><span class=o>.</span><span class=n>require_version</span><span class=p>(</span><span class=s1>&#39;Gst&#39;</span><span class=p>,</span> <span class=s1>&#39;1.0&#39;</span><span class=p>)</span>
<span class=kn>from</span> <span class=nn>gi.repository</span> <span class=kn>import</span> <span class=n>Gst</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
<span class=kn>import</span> <span class=nn>torch</span><span class=o>,</span> <span class=nn>torchvision</span>

<span class=n>frame_format</span><span class=p>,</span> <span class=n>pixel_bytes</span> <span class=o>=</span> <span class=s1>&#39;RGBA&#39;</span><span class=p>,</span> <span class=mi>4</span>
<span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s1>&#39;cuda&#39;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;cpu&#39;</span><span class=p>)</span>
<span class=n>detector</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>hub</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;NVIDIA/DeepLearningExamples:torchhub&#39;</span><span class=p>,</span> <span class=s1>&#39;nvidia_ssd&#39;</span><span class=p>,</span> <span class=n>model_math</span><span class=o>=</span><span class=s1>&#39;fp32&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
<span class=n>preprocess</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>()</span>

<span class=n>Gst</span><span class=o>.</span><span class=n>init</span><span class=p>()</span>
<span class=n>pipeline</span> <span class=o>=</span> <span class=n>Gst</span><span class=o>.</span><span class=n>parse_launch</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;&#39;&#39;
</span><span class=s1>    filesrc location=media/in.mp4 num-buffers=200 !
</span><span class=s1>    decodebin !
</span><span class=s1>    nvvideoconvert !
</span><span class=s1>    video/x-raw,format=</span><span class=si>{</span><span class=n>frame_format</span><span class=si>}</span><span class=s1> !
</span><span class=s1>    fakesink name=s
</span><span class=s1>&#39;&#39;&#39;</span><span class=p>)</span>

<span class=k>def</span> <span class=nf>on_frame_probe</span><span class=p>(</span><span class=n>pad</span><span class=p>,</span> <span class=n>info</span><span class=p>):</span>
    <span class=n>buf</span> <span class=o>=</span> <span class=n>info</span><span class=o>.</span><span class=n>get_buffer</span><span class=p>()</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;[</span><span class=si>{</span><span class=n>buf</span><span class=o>.</span><span class=n>pts</span> <span class=o>/</span> <span class=n>Gst</span><span class=o>.</span><span class=n>SECOND</span><span class=si>:</span><span class=s1>6.2f</span><span class=si>}</span><span class=s1>]&#39;</span><span class=p>)</span>

    <span class=n>image_tensor</span> <span class=o>=</span> <span class=n>buffer_to_image_tensor</span><span class=p>(</span><span class=n>buf</span><span class=p>,</span> <span class=n>pad</span><span class=o>.</span><span class=n>get_current_caps</span><span class=p>())</span>
    <span class=n>image_batch</span> <span class=o>=</span> <span class=n>image_tensor</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
        <span class=n>detections</span> <span class=o>=</span> <span class=n>detector</span><span class=p>(</span><span class=n>image_batch</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>

    <span class=k>return</span> <span class=n>Gst</span><span class=o>.</span><span class=n>PadProbeReturn</span><span class=o>.</span><span class=n>OK</span>

<span class=k>def</span> <span class=nf>buffer_to_image_tensor</span><span class=p>(</span><span class=n>buf</span><span class=p>,</span> <span class=n>caps</span><span class=p>):</span>
    <span class=n>caps_structure</span> <span class=o>=</span> <span class=n>caps</span><span class=o>.</span><span class=n>get_structure</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
    <span class=n>height</span><span class=p>,</span> <span class=n>width</span> <span class=o>=</span> <span class=n>caps_structure</span><span class=o>.</span><span class=n>get_value</span><span class=p>(</span><span class=s1>&#39;height&#39;</span><span class=p>),</span> <span class=n>caps_structure</span><span class=o>.</span><span class=n>get_value</span><span class=p>(</span><span class=s1>&#39;width&#39;</span><span class=p>)</span>

    <span class=n>is_mapped</span><span class=p>,</span> <span class=n>map_info</span> <span class=o>=</span> <span class=n>buf</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>Gst</span><span class=o>.</span><span class=n>MapFlags</span><span class=o>.</span><span class=n>READ</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>is_mapped</span><span class=p>:</span>
        <span class=k>try</span><span class=p>:</span>
            <span class=n>image_array</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ndarray</span><span class=p>(</span>
                <span class=p>(</span><span class=n>height</span><span class=p>,</span> <span class=n>width</span><span class=p>,</span> <span class=n>pixel_bytes</span><span class=p>),</span>
                <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>uint8</span><span class=p>,</span>
                <span class=n>buffer</span><span class=o>=</span><span class=n>map_info</span><span class=o>.</span><span class=n>data</span>
            <span class=p>)</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span> <span class=c1># extend array lifetime beyond subsequent unmap</span>
            <span class=k>return</span> <span class=n>preprocess</span><span class=p>(</span><span class=n>image_array</span><span class=p>[:,:,:</span><span class=mi>3</span><span class=p>])</span> <span class=c1># RGBA -&gt; RGB</span>
        <span class=k>finally</span><span class=p>:</span>
            <span class=n>buf</span><span class=o>.</span><span class=n>unmap</span><span class=p>(</span><span class=n>map_info</span><span class=p>)</span>

<span class=n>pipeline</span><span class=o>.</span><span class=n>get_by_name</span><span class=p>(</span><span class=s1>&#39;s&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>get_static_pad</span><span class=p>(</span><span class=s1>&#39;sink&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>add_probe</span><span class=p>(</span>
    <span class=n>Gst</span><span class=o>.</span><span class=n>PadProbeType</span><span class=o>.</span><span class=n>BUFFER</span><span class=p>,</span>
    <span class=n>on_frame_probe</span>
<span class=p>)</span>

<span class=n>pipeline</span><span class=o>.</span><span class=n>set_state</span><span class=p>(</span><span class=n>Gst</span><span class=o>.</span><span class=n>State</span><span class=o>.</span><span class=n>PLAYING</span><span class=p>)</span>

<span class=k>try</span><span class=p>:</span>
    <span class=k>while</span> <span class=kc>True</span><span class=p>:</span>
        <span class=n>msg</span> <span class=o>=</span> <span class=n>pipeline</span><span class=o>.</span><span class=n>get_bus</span><span class=p>()</span><span class=o>.</span><span class=n>timed_pop_filtered</span><span class=p>(</span>
            <span class=n>Gst</span><span class=o>.</span><span class=n>SECOND</span><span class=p>,</span>
            <span class=n>Gst</span><span class=o>.</span><span class=n>MessageType</span><span class=o>.</span><span class=n>EOS</span> <span class=o>|</span> <span class=n>Gst</span><span class=o>.</span><span class=n>MessageType</span><span class=o>.</span><span class=n>ERROR</span>
        <span class=p>)</span>
        <span class=k>if</span> <span class=n>msg</span><span class=p>:</span>
            <span class=n>text</span> <span class=o>=</span> <span class=n>msg</span><span class=o>.</span><span class=n>get_structure</span><span class=p>()</span><span class=o>.</span><span class=n>to_string</span><span class=p>()</span> <span class=k>if</span> <span class=n>msg</span><span class=o>.</span><span class=n>get_structure</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;&#39;</span>
            <span class=n>msg_type</span> <span class=o>=</span> <span class=n>Gst</span><span class=o>.</span><span class=n>message_type_get_name</span><span class=p>(</span><span class=n>msg</span><span class=o>.</span><span class=n>type</span><span class=p>)</span>
            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>msg</span><span class=o>.</span><span class=n>src</span><span class=o>.</span><span class=n>name</span><span class=si>}</span><span class=s1>: [</span><span class=si>{</span><span class=n>msg_type</span><span class=si>}</span><span class=s1>] </span><span class=si>{</span><span class=n>text</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
            <span class=k>break</span>
<span class=k>finally</span><span class=p>:</span>
    <span class=nb>open</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;logs/</span><span class=si>{</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>splitext</span><span class=p>(</span><span class=n>sys</span><span class=o>.</span><span class=n>argv</span><span class=p>[</span><span class=mi>0</span><span class=p>])[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=s1>.pipeline.dot&#39;</span><span class=p>,</span> <span class=s1>&#39;w&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>write</span><span class=p>(</span>
        <span class=n>Gst</span><span class=o>.</span><span class=n>debug_bin_to_dot_data</span><span class=p>(</span><span class=n>pipeline</span><span class=p>,</span> <span class=n>Gst</span><span class=o>.</span><span class=n>DebugGraphDetails</span><span class=o>.</span><span class=n>ALL</span><span class=p>)</span>
    <span class=p>)</span>
    <span class=n>pipeline</span><span class=o>.</span><span class=n>set_state</span><span class=p>(</span><span class=n>Gst</span><span class=o>.</span><span class=n>State</span><span class=o>.</span><span class=n>NULL</span><span class=p>)</span>
</code></pre></div>
<p>Some key points:</p>
<ul>
<li>In <code>buffer_to_image_tensor</code> we create a read-only mapping on the frame buffer memory then create a numpy array that points to the mapped memory.</li>
<li>We need to rearrange the image dimensions since Gstreamer has decoded to (height, width, channel) and this Pytorch model wants (channel, height, width).</li>
<li>In the pipeline below note the caps change from <code>video/x-raw(memory:NVVM)</code> to <code>video/x-raw</code> due to the newly added <code>nvvideoconvert</code> element. This element is transferring the decoded video buffer memory from GPU to CPU. The container has Gstreamer elements that support hardware video decoding so the <code>decodebin</code> element will use any compatible GPU to accelerate this. To make the decoded frames accessible for Gstreamer to map and read into Numpy we explicitly move them to host memory.</li>
</ul>
<details>
<summary>Output: frames_into_pytorch.py</summary>
<div>
<figure style="margin:2rem 0">
<img src=/posts/video-analytics-pytorch-pipeline/images/frames_into_pytorch.output.svg>
</figure>
</div>
</details>
<details>
<summary>Pipeline: frames_into_pytorch</summary>
<div>
<a href=/posts/video-analytics-pytorch-pipeline/images/frames_into_pytorch.pipeline.png>
<figure style="margin:2rem 0">
<img style=max-width:100%;width:auto;height:auto src=/posts/video-analytics-pytorch-pipeline/images/frames_into_pytorch.pipeline_hu72a2b805021a46ee07dd6bcb47dfba76_195243_800x0_resize_box_3.png width=800 height=49>
<figcaption><small></small></figcaption>
</figure>
</a>
</div>
</details>
<h2 id=conclusion>
Conclusion
<a class=anchor href=#conclusion>#</a>
</h2>
<p>The above samples are nowhere near production ready but they show that the fundamentals of running machine learning inference on video don&rsquo;t have to be hard. We hooked up a pipeline that can seamlessly process many different video encodings and formats. Further, we are using the regular &ldquo;research&rdquo; Pytorch runtime which gives us a lot of flexibility.</p>
<p>To give you a quick feel for performance: this pipeline runs at around 100 FPS on a 2080Ti at &lt;80% utilization. Caveats: we are not doing full pre-processing or post-processing, but on the other hand this pipeline is completely unoptimized. In my more advanced tuning post I add realistic preprocessing and postprocessing and <a href=/posts/video-analytics-pipeline-tuning/>make the performance awesome</a>.</p>
</article>
<article class="markdown social">
<hr>
<section class=social-share>
<ul class=share-icons>
<li>
<a href target=_blank class="share-btn newsletter"><svg class="widget-social__link-icon icon icon-mail" fill="#fff" width="24" height="24" viewBox="0 0 166.781 166.781"><g><g><g><path d="M163.451 70.046l-32.35-20.847c-.253-.161-.532-.222-.804-.312v-7.19c0-1.92-1.554-3.475-3.475-3.475H113.92L86.97 21.378c-1.126-.706-2.558-.706-3.685.0l-26.95 16.844H39.958c-1.92.0-3.475 1.554-3.475 3.475v7.188c-.272.09-.552.152-.804.314L3.329 70.046c-.991.641-1.592 1.741-1.592 2.921v90.339c0 1.92 1.554 3.475 3.475 3.475h156.356c1.92.0 3.475-1.554 3.475-3.475V72.968C165.043 71.787 164.442 70.688 163.451 70.046zM85.128 28.423l15.681 9.799H69.447l15.681-9.799zM43.433 45.171h79.915v78.178c0 .01.006.018.006.029l-11.754 7.137-28.284-15.427c-1.055-.57-2.338-.567-3.386.034l-25.81 14.749-10.692-6.492c0-.01.006-.018.006-.028L43.433 45.171zM8.687 74.861l27.796-17.91v62.212L8.687 102.285V74.861zm0 35.551 38.537 23.397L8.687 155.831V110.412zm7.002 49.421 66.005-37.715 69.145 37.715H15.689zm142.405-3.959L118.65 134.36l39.444-23.949v45.463zm0-53.589-27.797 16.877V56.951l27.797 17.911v27.423z"/><path d="M57.331 79.917h41.695c1.92.0 3.475-1.554 3.475-3.475V55.595c0-1.92-1.554-3.475-3.475-3.475H57.331c-1.92.0-3.475 1.554-3.475 3.475v20.847C53.856 78.363 55.411 79.917 57.331 79.917zm3.474-20.848h34.746v13.898H60.805V59.069z"/><rect x="53.856" y="86.866" width="55.593" height="6.949"/><rect x="53.856" y="100.765" width="55.593" height="6.949"/><path d="M147.67 41.697c.889.0 1.778-.339 2.457-1.018l12.283-12.283c1.357-1.357 1.357-3.556.0-4.913-1.357-1.358-3.556-1.357-4.913.0l-12.283 12.283c-1.357 1.357-1.357 3.556.0 4.913C145.892 41.358 146.781 41.697 147.67 41.697z"/><path d="M16.654 40.679c.679.679 1.568 1.018 2.457 1.018s1.778-.339 2.457-1.018c1.357-1.357 1.357-3.556.0-4.913L9.284 23.483c-1.357-1.357-3.556-1.357-4.913.0s-1.357 3.556.0 4.913L16.654 40.679z"/><path d="M118.584 24.076c.421.17.859.247 1.289.247 1.378.0 2.684-.825 3.227-2.185l6.949-17.373c.713-1.781-.156-3.804-1.937-4.516-1.764-.709-3.804.149-4.516 1.937l-6.949 17.373C115.934 21.341 116.802 23.364 118.584 24.076z"/><path d="M47.155 22.139c.543 1.361 1.849 2.185 3.227 2.185.431.0.869-.078 1.289-.248 1.781-.713 2.65-2.735 1.937-4.516L46.659 2.187C45.946.399 43.911-.46 42.143.25c-1.781.713-2.65 2.735-1.937 4.516l6.949 17.373z"/></g></g></g></svg>
<p>Newsletter</p>
</a>
</li>
<li class=share-label>
<script async data-uid=d104ecfe6c src=https://paulbridger.ck.page/d104ecfe6c/index.js></script>
</li>
</ul>
<ul class=share-icons>
<li>
<a href="https://twitter.com/intent/follow?ref_src=twsrc%5Etfw&region=follow_link&screen_name=paul_bridger&tw_p=followbutton" target=_blank rel=noopener aria-label="Follow on Twitter" class="share-btn twitter"><svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path fill="#fff" d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5.0-78.8 35.3-78.8 78.8.0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3C20 26 16.1 39.6 16.1 54c0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1.0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4.0-12.6-.4-18.8-1.1C34.9 299 76.3 312 120.8 312c144.9.0 224.1-120 224.1-224.1.0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
<p>Twitter</p>
</a>
</li>
<li class=share-label>
<a href="https://twitter.com/intent/follow?ref_src=twsrc%5Etfw&region=follow_link&screen_name=paul_bridger&tw_p=followbutton" target=_blank rel=noopener aria-label="Follow on Twitter">
Follow on Twitter
</a>
</li>
</ul>
<ul class=share-icons>
<li>
<a href=https://www.linkedin.com/in/paulbridger/ target=_blank rel=noopener aria-label="Connect on LinkedIn" class="share-btn linkedin"><svg class="widget-social__link-icon icon icon-linkedin" width="24" height="24" viewBox="0 0 352 352"><path fill="#fff" d="M0 40v272c0 21.9 18.1 40 40 40h272c21.9.0 40-18.1 40-40V40c0-21.9-18.1-40-40-40H40C18.1.0.0 18.1.0 40zm312-8c4.6.0 8 3.4 8 8v272c0 4.6-3.4 8-8 8H40c-4.6.0-8-3.4-8-8V40c0-4.6 3.4-8 8-8H312zM59.5 87c0 15.2 12.3 27.5 27.5 27.5s27.5-12.3 27.5-27.5S102.2 59.5 87 59.5 59.5 71.8 59.5 87zM187 157h-1v-21h-45v152h47v-75c0-19.8 3.9-39 28.5-39 24.2.0 24.5 22.4 24.5 40v74h47v-83.5c0-40.9-8.7-72-56.5-72-23 0-38.2 12.6-44.5 24.5zM64 288h47.5V136H64V288z"/></svg>
<p>LinkedIn</p>
</a>
</li>
<li class=share-label>
<a href=https://www.linkedin.com/in/paulbridger/ target=_blank rel=noopener aria-label="Connect on LinkedIn">
Connect on LinkedIn
</a>
</li>
</ul>
<ul class=share-icons>
<li>
<a href=mailto:paul@paulbridger.com target=_blank class="share-btn email"><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path fill="#fff" d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg>
<p>Email</p>
</a>
</li>
<li class=share-label>
<a href=mailto:paul@paulbridger.com target=_blank>
Contact by Email
</a>
</li>
</ul>
</section>
</article>
<footer class=book-footer>
<div class="flex flex-wrap justify-between">
</div>
<script>(function(){function a(c){const a=window.getSelection(),b=document.createRange();b.selectNodeContents(c),a.removeAllRanges(),a.addRange(b)}document.querySelectorAll("pre code").forEach(b=>{b.addEventListener("click",function(c){a(b.parentElement),navigator.clipboard&&navigator.clipboard.writeText(b.parentElement.textContent)})})})()</script>
<hr>
<b>&copy; Paul Bridger 2020</b>
</footer>
<div class=book-comments>
</div>
<label for=menu-control class="hidden book-menu-overlay"></label>
</div>
<aside class=book-toc>
<div class=book-toc-content>
<div class=book-toc-div>
<nav id=TableOfContents>
<ul>
<li><a href=#intro>Intro</a></li>
<li><a href=#step-0-baseline-cli-gstreamer-pipeline>Step 0: Baseline CLI Gstreamer pipeline</a></li>
<li><a href=#step-1-get-frames-into-python>Step 1: Get frames into Python</a></li>
<li><a href=#step-2-get-frames-into-pytorch>Step 2: Get frames into Pytorch</a></li>
<li><a href=#conclusion>Conclusion</a></li>
</ul>
</nav>
<nav>
</nav>
</div>
</div>
</aside>
</main>
</body>
</html>