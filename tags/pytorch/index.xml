<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Pytorch on paulbridger.com</title><link>https://paulbridger.com/tags/pytorch/</link><description>Recent content in Pytorch on paulbridger.com</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 20 Jul 2023 08:43:23 +0200</lastBuildDate><atom:link href="https://paulbridger.com/tags/pytorch/index.xml" rel="self" type="application/rss+xml"/><item><title>PyTorch Memory Tuning</title><link>https://paulbridger.com/posts/pytorch-memory-tuning/</link><pubDate>Thu, 20 Jul 2023 08:43:23 +0200</pubDate><guid>https://paulbridger.com/posts/pytorch-memory-tuning/</guid><description/></item><item><title>PyTorch Performance Features and How They Interact</title><link>https://paulbridger.com/posts/pytorch-tuning-tips/</link><pubDate>Fri, 14 Apr 2023 08:43:23 +0200</pubDate><guid>https://paulbridger.com/posts/pytorch-tuning-tips/</guid><description>PyTorch in 2023 is a complex beast, with many great performance features hidden away. Simple top-N lists are weak content, so I&amp;rsquo;ve empirically tested the most important PyTorch tuning techniques and settings in all combinations. I&amp;rsquo;ve benchmarked inference across a handful of different model architectures and sizes, different versions of PyTorch and even different Docker containers.</description></item><item><title>A Simple and Flexible Pytorch Video Pipeline</title><link>https://paulbridger.com/posts/video-analytics-pytorch-pipeline/</link><pubDate>Wed, 23 Sep 2020 18:21:00 +0200</pubDate><guid>https://paulbridger.com/posts/video-analytics-pytorch-pipeline/</guid><description>Taking machine learning models into production for video analytics doesn&amp;rsquo;t have to be hard. A pipeline with reasonable efficiency can be created very quickly just by plugging together the right libraries. In this post we&amp;rsquo;ll create a video pipeline with a focus on flexibility and simplicity using two main libraries: &lt;a href="https://gstreamer.freedesktop.org/">Gstreamer&lt;/a> and &lt;a href="https://pytorch.org">Pytorch&lt;/a>.</description></item><item><title>Object Detection from 9 FPS to 650 FPS in 6 Steps</title><link>https://paulbridger.com/posts/video-analytics-pipeline-tuning/</link><pubDate>Wed, 30 Sep 2020 08:43:23 +0200</pubDate><guid>https://paulbridger.com/posts/video-analytics-pipeline-tuning/</guid><description>Making code run fast on GPUs requires a very different approach to making code run fast on CPUs because the hardware architecture is fundamentally different. Machine learning engineers of all kinds should care about squeezing performance from their models and hardware â€” not just for production purposes, but also for research and training. In research as in development, a fast iteration loop leads to faster improvement. This article is a practical deep dive into making a specific deep learning model (&lt;a href="https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/">Nvidia&amp;rsquo;s SSD300&lt;/a>) run fast on a powerful GPU server, but the general principles apply to all GPU programming.</description></item><item><title>Object Detection at 1840 FPS with TorchScript, TensorRT and DeepStream</title><link>https://paulbridger.com/posts/video-analytics-deepstream-pipeline/</link><pubDate>Sat, 17 Oct 2020 08:43:23 +0200</pubDate><guid>https://paulbridger.com/posts/video-analytics-deepstream-pipeline/</guid><description>In this article we take performance of the SSD300 model even further, leaving Python behind and moving towards true production deployment technologies: TorchScript, TensorRT and DeepStream. We also identify and understand several limitations in Nvidia&amp;rsquo;s DeepStream framework, and then remove them by modifying how the &lt;code>nvinfer&lt;/code> element works.</description></item><item><title>Object Detection at 2530 FPS with TensorRT and 8-Bit Quantization</title><link>https://paulbridger.com/posts/tensorrt-object-detection-quantized/</link><pubDate>Thu, 31 Dec 2020 08:43:23 +0200</pubDate><guid>https://paulbridger.com/posts/tensorrt-object-detection-quantized/</guid><description>This article is a deep dive into the techniques needed to get SSD300 object detection throughput to 2530 FPS. We will rewrite Pytorch model code, perform &lt;a href="https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon">ONNX graph surgery&lt;/a>, optimize &lt;a href="https://github.com/NVIDIA/TensorRT/tree/master/plugin/batchedNMSPlugin">a TensorRT plugin&lt;/a> and finally we&amp;rsquo;ll quantize the model to an 8-bit representation. We will also examine divergence from the accuracy of the full-precision model.</description></item><item><title>Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification</title><link>https://paulbridger.com/posts/mastering-torchscript/</link><pubDate>Thu, 29 Oct 2020 08:43:23 +0200</pubDate><guid>https://paulbridger.com/posts/mastering-torchscript/</guid><description>TorchScript is one of the most important parts of the Pytorch ecosystem, allowing portable, efficient and nearly seamless deployment. With just a few lines of &lt;code>torch.jit&lt;/code> code and some simple model changes you can export an asset that runs anywhere &lt;code>libtorch&lt;/code> does. It&amp;rsquo;s an important toolset to master if you want to run your models outside the lab at high efficiency. This article is a collection of topics going beyond the basics of your first export.</description></item></channel></rss>