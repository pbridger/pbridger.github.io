<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>TensorRT on paulbridger.com</title><link>https://paulbridger.com/tags/tensorrt/</link><description>Recent content in TensorRT on paulbridger.com</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 24 Jun 2021 00:02:03 +0200</lastBuildDate><atom:link href="https://paulbridger.com/tags/tensorrt/index.xml" rel="self" type="application/rss+xml"/><item><title>Systematic Machine Learning Optimization using Nsight Systems</title><link>https://paulbridger.com/posts/nsight-systems-systematic-optimization/</link><pubDate>Thu, 24 Jun 2021 00:02:03 +0200</pubDate><guid>https://paulbridger.com/posts/nsight-systems-systematic-optimization/</guid><description>This article is a high-level introduction to an efficient worfklow for optimizing runtime performance of machine learning systems running on the GPU. Using traces from Nsight Systems to show real production scenarios, I introduce a set of common utilization patterns and outline effective approaches to improve performance.</description></item><item><title>Object Detection at 1840 FPS with TorchScript, TensorRT and DeepStream</title><link>https://paulbridger.com/posts/video-analytics-deepstream-pipeline/</link><pubDate>Sat, 17 Oct 2020 08:43:23 +0200</pubDate><guid>https://paulbridger.com/posts/video-analytics-deepstream-pipeline/</guid><description>In this article we take performance of the SSD300 model even further, leaving Python behind and moving towards true production deployment technologies: TorchScript, TensorRT and DeepStream. We also identify and understand several limitations in Nvidia&amp;rsquo;s DeepStream framework, and then remove them by modifying how the &lt;code>nvinfer&lt;/code> element works.</description></item><item><title>Object Detection at 2530 FPS with TensorRT and 8-Bit Quantization</title><link>https://paulbridger.com/posts/tensorrt-object-detection-quantized/</link><pubDate>Thu, 31 Dec 2020 08:43:23 +0200</pubDate><guid>https://paulbridger.com/posts/tensorrt-object-detection-quantized/</guid><description>This article is a deep dive into the techniques needed to get SSD300 object detection throughput to 2530 FPS. We will rewrite Pytorch model code, perform &lt;a href="https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon">ONNX graph surgery&lt;/a>, optimize &lt;a href="https://github.com/NVIDIA/TensorRT/tree/master/plugin/batchedNMSPlugin">a TensorRT plugin&lt;/a> and finally we&amp;rsquo;ll quantize the model to an 8-bit representation. We will also examine divergence from the accuracy of the full-precision model.</description></item><item><title>Mastering TorchScript: Tracing vs Scripting, Device Pinning, Direct Graph Modification</title><link>https://paulbridger.com/posts/mastering-torchscript/</link><pubDate>Thu, 29 Oct 2020 08:43:23 +0200</pubDate><guid>https://paulbridger.com/posts/mastering-torchscript/</guid><description>TorchScript is one of the most important parts of the Pytorch ecosystem, allowing portable, efficient and nearly seamless deployment. With just a few lines of &lt;code>torch.jit&lt;/code> code and some simple model changes you can export an asset that runs anywhere &lt;code>libtorch&lt;/code> does. It&amp;rsquo;s an important toolset to master if you want to run your models outside the lab at high efficiency. This article is a collection of topics going beyond the basics of your first export.</description></item></channel></rss>